{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the output from the derivatives I derived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quats_s_to_covariance passed gradcheck\n",
      "tensor([[  6.3256,   4.5076,   8.3424, -10.0920]])\n",
      "tensor([[-0.1481, 23.7990, 52.3883]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The purpose of this file is to go from quaternions (normalized) and scales (after activation) to the 3D covariance matrix\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "from splat.utils import build_rotation\n",
    "\n",
    "\n",
    "def d_r_wrt_qr(quats: torch.Tensor, n: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the derivative of m wrt quats\n",
    "    quats is nx4 tensor\n",
    "    shape is nx3 tensor\n",
    "    \"\"\"\n",
    "    qr = quats[:, 0]\n",
    "    qi = quats[:, 1]\n",
    "    qj = quats[:, 2]\n",
    "    qk = quats[:, 3]\n",
    "\n",
    "    derivative = torch.zeros((n, 3, 3))\n",
    "    derivative[:, 0, 1] = -qk\n",
    "    derivative[:, 0, 2] = qj\n",
    "    derivative[:, 1, 0] = qk\n",
    "    derivative[:, 1, 2] = -qi\n",
    "    derivative[:, 2, 0] = -qj\n",
    "    derivative[:, 2, 1] = qi\n",
    "\n",
    "    return 2 * derivative\n",
    "\n",
    "\n",
    "def d_r_wrt_qi(quats: torch.Tensor, n: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the derivative of m wrt quats\n",
    "    quats is nx4 tensor\n",
    "    shape is nx3 tensor\n",
    "    \"\"\"\n",
    "    qr = quats[:, 0]\n",
    "    qi = quats[:, 1]\n",
    "    qj = quats[:, 2]\n",
    "    qk = quats[:, 3]\n",
    "\n",
    "    derivative = torch.zeros((n, 3, 3))\n",
    "    derivative[:, 0, 1] = qj\n",
    "    derivative[:, 0, 2] = qk\n",
    "    derivative[:, 1, 0] = qj\n",
    "    derivative[:, 1, 1] = -2 * qi\n",
    "    derivative[:, 1, 2] = -qr\n",
    "    derivative[:, 2, 0] = qk\n",
    "    derivative[:, 2, 1] = qr\n",
    "    derivative[:, 2, 2] = -2 * qi\n",
    "    return 2 * derivative\n",
    "\n",
    "\n",
    "def d_r_wrt_qj(quats: torch.Tensor, n: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the derivative of m wrt quats\n",
    "    quats is nx4 tensor\n",
    "    shape is nx3 tensor\n",
    "    \"\"\"\n",
    "    qr = quats[:, 0]\n",
    "    qi = quats[:, 1]\n",
    "    qj = quats[:, 2]\n",
    "    qk = quats[:, 3]\n",
    "\n",
    "    derivative = torch.zeros((n, 3, 3))\n",
    "    derivative[:, 0, 0] = -2 * qj\n",
    "    derivative[:, 0, 1] = qi\n",
    "    derivative[:, 0, 2] = qr\n",
    "    derivative[:, 1, 0] = qi\n",
    "    derivative[:, 1, 1] = 0\n",
    "    derivative[:, 1, 2] = qk\n",
    "    derivative[:, 2, 0] = -qr\n",
    "    derivative[:, 2, 1] = qk\n",
    "    derivative[:, 2, 2] = -2 * qj\n",
    "    return 2 * derivative\n",
    "\n",
    "\n",
    "def d_r_wrt_qk(quats: torch.Tensor, n: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the derivative of m wrt quats\n",
    "    quats is nx4 tensor\n",
    "    shape is nx3 tensor\n",
    "    \"\"\"\n",
    "    qr = quats[:, 0]\n",
    "    qi = quats[:, 1]\n",
    "    qj = quats[:, 2]\n",
    "    qk = quats[:, 3]\n",
    "\n",
    "    derivative = torch.zeros((n, 3, 3))\n",
    "    derivative[:, 0, 0] = -2 * qk\n",
    "    derivative[:, 0, 1] = -qr\n",
    "    derivative[:, 0, 2] = qi\n",
    "    derivative[:, 1, 0] = qr\n",
    "    derivative[:, 1, 1] = -2*qk\n",
    "    derivative[:, 1, 2] = qj\n",
    "    derivative[:, 2, 0] = qi\n",
    "    derivative[:, 2, 1] = qj\n",
    "    derivative[:, 2, 2] = 0\n",
    "    return 2 * derivative\n",
    "\n",
    "class quatsToR(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, quats: torch.Tensor) -> torch.Tensor:\n",
    "        ctx.save_for_backward(quats)\n",
    "        R = build_rotation(quats, normalize=False)\n",
    "        return R\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        quats = ctx.saved_tensors[0]\n",
    "        deriv_wrt_qr = d_r_wrt_qr(quats, quats.shape[0])\n",
    "        deriv_wrt_qi = d_r_wrt_qi(quats, quats.shape[0])\n",
    "        deriv_wrt_qj = d_r_wrt_qj(quats, quats.shape[0])\n",
    "        deriv_wrt_qk = d_r_wrt_qk(quats, quats.shape[0])\n",
    "        \n",
    "        deriv_wrt_qr = (grad_output * deriv_wrt_qr).sum(dim=(1, 2), keepdim=True).squeeze(2)\n",
    "        deriv_wrt_qi = (grad_output * deriv_wrt_qi).sum(dim=(1, 2), keepdim=True).squeeze(2)\n",
    "        deriv_wrt_qj = (grad_output * deriv_wrt_qj).sum(dim=(1, 2), keepdim=True).squeeze(2)\n",
    "        deriv_wrt_qk = (grad_output * deriv_wrt_qk).sum(dim=(1, 2), keepdim=True).squeeze(2)\n",
    "        return torch.cat([deriv_wrt_qr, deriv_wrt_qi, deriv_wrt_qj, deriv_wrt_qk], dim=1)\n",
    "    \n",
    "class s_to_S(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, s: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        s is nx3 tensor\n",
    "        S is nx3x3 tensor\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(s)\n",
    "        S = torch.diag_embed(s)\n",
    "        return S\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        grad_output is nx3x3 tensor\n",
    "        s is nx3 tensor\n",
    "        \"\"\"\n",
    "        s = ctx.saved_tensors[0]\n",
    "        deriv_wrt_s1 = grad_output[:, 0:1, 0:1].view(-1, 1)\n",
    "        deriv_wrt_s2 = grad_output[:, 1:2, 1:2].view(-1, 1)\n",
    "        deriv_wrt_s3 = grad_output[:, 2:3, 2:3].view(-1, 1)\n",
    "        return torch.cat([deriv_wrt_s1, deriv_wrt_s2, deriv_wrt_s3], dim=1)\n",
    "    \n",
    "class R_S_to_M(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, R: torch.Tensor, S: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        R is nx3x3 tensor\n",
    "        S is nx3x3 tensor\n",
    "        M is nx3x3 tensor\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(R, S)\n",
    "        M = torch.bmm(R, S)\n",
    "        return M\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        R, S = ctx.saved_tensors\n",
    "        deriv_wrt_R = torch.bmm(grad_output, S.transpose(1, 2))\n",
    "        deriv_wrt_S = torch.bmm(R.transpose(1, 2), grad_output)\n",
    "        \n",
    "        return deriv_wrt_R, deriv_wrt_S\n",
    "    \n",
    "class M_to_covariance(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, M: torch.Tensor) -> torch.Tensor:\n",
    "        ctx.save_for_backward(M)\n",
    "        covariance = M.pow(2)\n",
    "        return covariance\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"So the grad output should be nx3x3 and M is nx3x3\"\"\"\n",
    "        # TODO I am unsure how to use this tensor - \n",
    "        # the paper says something different than what I am getting\n",
    "        M = ctx.saved_tensors[0]\n",
    "        deriv_wrt_M = 2 * grad_output * M\n",
    "        return deriv_wrt_M\n",
    "\n",
    "\n",
    "class quats_s_to_covariance2(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, quats: torch.Tensor, s: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        quats is nx4 tensor\n",
    "        s is nx3 tensor\n",
    "        \n",
    "        Returns a nx3x3 covariance matrix\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(quats, s)\n",
    "        R = quatsToR.apply(quats)\n",
    "        S = s_to_S.apply(s)\n",
    "        M = R_S_to_M.apply(R, S)\n",
    "        covariance = M_to_covariance.apply(M)\n",
    "        return covariance\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        grad_output is nx3x3 tensor\n",
    "        Returns a tuple of the derivatives of the covariance matrix wrt the \n",
    "        quats (n, 4) and s (n, 3)\n",
    "        \"\"\"\n",
    "        quats, s = ctx.saved_tensors\n",
    "        deriv_wrt_quats = quatsToR.backward(quats, grad_output)\n",
    "        deriv_wrt_s = s_to_S.backward(s, grad_output)\n",
    "        return deriv_wrt_quats, deriv_wrt_s\n",
    "    \n",
    "    \n",
    "\n",
    "def quats_s_to_covariance(quats: torch.Tensor, s: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Function to compute the covariance matrix.\n",
    "    It assumes that the quats are normalized and the s is post activation\n",
    "    \n",
    "    quats is nx4 tensor\n",
    "    s is nx3 tensor\n",
    "    \n",
    "    Returns a nx3x3 covariance matrix\n",
    "    \"\"\"\n",
    "    R = quatsToR.apply(quats)\n",
    "    S = s_to_S.apply(s)\n",
    "    M = R_S_to_M.apply(R, S)\n",
    "    return M_to_covariance.apply(M)\n",
    "    \n",
    "\n",
    "\n",
    "starting_r = torch.tensor([[1.0, 2.0, 3.0, 4.0]]).requires_grad_(True)\n",
    "starting_s = torch.tensor([[1.0, 2.0, 3.0]]).requires_grad_(True)\n",
    "r = starting_r / torch.norm(starting_r)\n",
    "\n",
    "output_cov = quats_s_to_covariance(r, starting_s)\n",
    "if gradcheck(quats_s_to_covariance, (r.double(), starting_s.double()), eps=1e-4, atol=1e-4):\n",
    "    print(\"quats_s_to_covariance passed gradcheck\")\n",
    "else:\n",
    "    print(\"quats_s_to_covariance failed gradcheck\")\n",
    "\n",
    "target = torch.eye(3).unsqueeze(0)\n",
    "loss = (output_cov - target).pow(2).sum()\n",
    "loss.backward()\n",
    "print(starting_r.grad)\n",
    "print(starting_s.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test against autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output covariance:  tensor([[[0.4444, 0.0711, 4.8400],\n",
      "         [0.4444, 0.4444, 4.0000],\n",
      "         [0.1111, 3.4844, 0.1600]]], grad_fn=<PowBackward0>)\n",
      "quats_s_to_covariance passed gradcheck\n",
      "tensor([[  6.3256,   4.5076,   8.3424, -10.0920]])\n",
      "tensor([[-0.1481, 23.7990, 52.3883]])\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
