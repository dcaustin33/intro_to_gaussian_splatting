{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We implement and then we check each gradient so when we daisy chain we are good'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"We implement and then we check each gradient so when we daisy chain we are good\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradcheck passing:  True\n",
      "My grads:  tensor([[20., 20., 20.]], dtype=torch.float64) None tensor([[24.]], dtype=torch.float64)\n",
      "Autograd grads:  tensor([[20., 20., 20.]], dtype=torch.float64) None tensor([[24.]], dtype=torch.float64)\n",
      "My grads:  tensor([[20., 20., 20.],\n",
      "        [30., 30., 30.]], dtype=torch.float64) None tensor([[24.],\n",
      "        [75.]], dtype=torch.float64)\n",
      "Autograd grads:  tensor([[20., 20., 20.],\n",
      "        [30., 30., 30.]], dtype=torch.float64) None tensor([[24.],\n",
      "        [75.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "from splat.utils import build_rotation\n",
    "\n",
    "\n",
    "class final_color(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, color: torch.Tensor, current_T: torch.Tensor, alpha: torch.Tensor):\n",
    "        \"\"\"Color is a nx3 tensor, weight is a nx1 tensor, alpha is a nx1 tensor\"\"\"\n",
    "        ctx.save_for_backward(color, current_T, alpha)\n",
    "        return color * current_T * alpha\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        \"\"\"Output of forward is a nx3 tensor so the grad_output is a nx3 tensor\"\"\"\n",
    "        color, current_T, alpha = ctx.saved_tensors\n",
    "        grad_color = grad_output * current_T * alpha\n",
    "        grad_alpha = (grad_output * color * current_T).sum(dim=1, keepdim=True)\n",
    "        return grad_color, None, grad_alpha\n",
    "    \n",
    "def autograd_test(color: torch.Tensor, current_T: torch.Tensor, alpha: torch.Tensor):\n",
    "    return color * current_T * alpha\n",
    "\n",
    "# first we check with gradcheck\n",
    "color = torch.tensor([[1.0, 2.0, 3.0]], requires_grad=True, dtype=torch.float64)\n",
    "current_T = torch.tensor([[4.0]], dtype=torch.float64)\n",
    "alpha = torch.tensor([[5.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "print(\"gradcheck passing: \", gradcheck(final_color.apply, (color, current_T, alpha)))\n",
    "\n",
    "# then we check with autograd\n",
    "color = torch.tensor([[1.0, 2.0, 3.0]], requires_grad=True, dtype=torch.float64)\n",
    "current_T = torch.tensor([[4.0]], dtype=torch.float64)\n",
    "alpha = torch.tensor([[5.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "output = final_color.apply(color, current_T, alpha)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(\"My grads: \", color.grad, current_T.grad, alpha.grad)\n",
    "\n",
    "# then we check with autograd\n",
    "color = torch.tensor([[1.0, 2.0, 3.0]], requires_grad=True, dtype=torch.float64)\n",
    "current_T = torch.tensor([[4.0]], dtype=torch.float64)\n",
    "alpha = torch.tensor([[5.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "output = autograd_test(color, current_T, alpha)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(\"Autograd grads: \", color.grad, current_T.grad, alpha.grad)\n",
    "\n",
    "\n",
    "# finally we test with multiple dimensions\n",
    "color = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], requires_grad=True, dtype=torch.float64)\n",
    "current_T = torch.tensor([[4.0], [5.0]], dtype=torch.float64)\n",
    "alpha = torch.tensor([[5.0], [6.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "output = final_color.apply(color, current_T, alpha)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(\"My grads: \", color.grad, current_T.grad, alpha.grad)\n",
    "\n",
    "color = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], requires_grad=True, dtype=torch.float64)\n",
    "current_T = torch.tensor([[4.0], [5.0]], dtype=torch.float64)\n",
    "alpha = torch.tensor([[5.0], [6.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "output = autograd_test(color, current_T, alpha)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(\"Autograd grads: \", color.grad, current_T.grad, alpha.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradcheck passing:  True\n",
      "My grads:  tensor([[0.6225]], dtype=torch.float64) tensor([[0.4700]], dtype=torch.float64)\n",
      "Autograd grads:  tensor([[0.6225]], dtype=torch.float64) tensor([[0.4700]], dtype=torch.float64)\n",
      "My grads (multiple dimensions):  tensor([[0.6225],\n",
      "        [0.8176]], dtype=torch.float64) tensor([[0.4700],\n",
      "        [0.4474]], dtype=torch.float64)\n",
      "Autograd grads (multiple dimensions):  tensor([[0.6225],\n",
      "        [0.8176]], dtype=torch.float64) tensor([[0.4700],\n",
      "        [0.4474]], dtype=torch.float64)\n",
      "My grads (edge cases):  tensor([[9.9995e-01],\n",
      "        [4.5398e-05]], dtype=torch.float64) tensor([[4.5396e-10],\n",
      "        [4.5396e+00]], dtype=torch.float64)\n",
      "Autograd grads (edge cases):  tensor([[9.9995e-01],\n",
      "        [4.5398e-05]], dtype=torch.float64) tensor([[4.5396e-10],\n",
      "        [4.5396e+00]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "class get_alpha(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, gaussian_strength: torch.Tensor, unactivated_opacity: torch.Tensor):\n",
    "        \"\"\"Gaussian strength is a nx1 tensor, unactivated opacity is a nx1 tensor\"\"\"\n",
    "        ctx.save_for_backward(gaussian_strength, unactivated_opacity)\n",
    "        activated_opacity = torch.sigmoid(unactivated_opacity)\n",
    "        return gaussian_strength * activated_opacity\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        \"\"\"Output of forward is a nx1 tensor so the grad_output is a nx1 tensor\"\"\"\n",
    "        gaussian_strength, unactivated_opacity = ctx.saved_tensors\n",
    "        derivative_sigmoid = torch.sigmoid(unactivated_opacity) * (1 - torch.sigmoid(unactivated_opacity))\n",
    "        grad_gaussian_strength = grad_output * torch.sigmoid(unactivated_opacity)\n",
    "        grad_unactivated_opacity = grad_output * gaussian_strength * derivative_sigmoid\n",
    "        return grad_gaussian_strength, grad_unactivated_opacity\n",
    "    \n",
    "# Define a test function using torch's autograd\n",
    "def autograd_test(gaussian_strength: torch.Tensor, unactivated_opacity: torch.Tensor):\n",
    "    activated_opacity = torch.sigmoid(unactivated_opacity)\n",
    "    return gaussian_strength * activated_opacity\n",
    "\n",
    "# 1. Gradcheck for numerical gradient correctness\n",
    "gaussian_strength = torch.tensor([[2.0]], requires_grad=True, dtype=torch.float64)\n",
    "unactivated_opacity = torch.tensor([[0.5]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "print(\"gradcheck passing: \", gradcheck(get_alpha.apply, (gaussian_strength, unactivated_opacity)))\n",
    "\n",
    "# 2. Verify backward computation with autograd\n",
    "gaussian_strength = torch.tensor([[2.0]], requires_grad=True, dtype=torch.float64)\n",
    "unactivated_opacity = torch.tensor([[0.5]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "output = get_alpha.apply(gaussian_strength, unactivated_opacity)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(\"My grads: \", gaussian_strength.grad, unactivated_opacity.grad)\n",
    "\n",
    "# Compare with autograd\n",
    "gaussian_strength = torch.tensor([[2.0]], requires_grad=True, dtype=torch.float64)\n",
    "unactivated_opacity = torch.tensor([[0.5]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "output = autograd_test(gaussian_strength, unactivated_opacity)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(\"Autograd grads: \", gaussian_strength.grad, unactivated_opacity.grad)\n",
    "\n",
    "# 3. Test with multiple dimensions\n",
    "gaussian_strength = torch.tensor([[2.0], [3.0]], requires_grad=True, dtype=torch.float64)\n",
    "unactivated_opacity = torch.tensor([[0.5], [1.5]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "output = get_alpha.apply(gaussian_strength, unactivated_opacity)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(\"My grads (multiple dimensions): \", gaussian_strength.grad, unactivated_opacity.grad)\n",
    "\n",
    "# Compare with autograd\n",
    "gaussian_strength = torch.tensor([[2.0], [3.0]], requires_grad=True, dtype=torch.float64)\n",
    "unactivated_opacity = torch.tensor([[0.5], [1.5]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "output = autograd_test(gaussian_strength, unactivated_opacity)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(\"Autograd grads (multiple dimensions): \", gaussian_strength.grad, unactivated_opacity.grad)\n",
    "\n",
    "# 4. Edge case: Very large and very small values\n",
    "gaussian_strength = torch.tensor([[1e-5], [1e5]], requires_grad=True, dtype=torch.float64)\n",
    "unactivated_opacity = torch.tensor([[10.0], [-10.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "output = get_alpha.apply(gaussian_strength, unactivated_opacity)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(\"My grads (edge cases): \", gaussian_strength.grad, unactivated_opacity.grad)\n",
    "\n",
    "gaussian_strength = torch.tensor([[1e-5], [1e5]], requires_grad=True, dtype=torch.float64)\n",
    "unactivated_opacity = torch.tensor([[10.0], [-10.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "output = autograd_test(gaussian_strength, unactivated_opacity)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(\"Autograd grads (edge cases): \", gaussian_strength.grad, unactivated_opacity.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed.\n",
      "My grads match autograd grads (single value).\n",
      "My grads match autograd grads (multiple dimensions).\n",
      "My grads match autograd grads (edge cases).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "\n",
    "class gaussian_exp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, gaussian_weight: torch.Tensor):\n",
    "        ctx.save_for_backward(gaussian_weight)\n",
    "        return torch.exp(gaussian_weight)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        gaussian_weight, = ctx.saved_tensors\n",
    "        grad_gaussian_weight = grad_output * torch.exp(gaussian_weight)\n",
    "        return grad_gaussian_weight\n",
    "\n",
    "# Define a test function using torch's autograd\n",
    "def autograd_test(gaussian_weight: torch.Tensor):\n",
    "    return torch.exp(gaussian_weight)\n",
    "\n",
    "# Helper function to compare gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# Test cases\n",
    "def run_tests():\n",
    "    tolerance = 1e-8\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    gaussian_weight = torch.tensor([[1.0]], requires_grad=True, dtype=torch.float64)\n",
    "    gradcheck_passed = gradcheck(gaussian_exp.apply, (gaussian_weight,))\n",
    "    assert gradcheck_passed, \"Gradcheck failed!\"\n",
    "    print(\"Gradcheck passed.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd (single value)\n",
    "    gaussian_weight = torch.tensor([[1.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = gaussian_exp.apply(gaussian_weight)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = gaussian_weight.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    gaussian_weight.grad.zero_()\n",
    "    output = autograd_test(gaussian_weight)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = gaussian_weight.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"My grads do not match autograd grads (single value)!\"\n",
    "    print(\"My grads match autograd grads (single value).\")\n",
    "\n",
    "    # 3. Test with multiple dimensions\n",
    "    gaussian_weight = torch.tensor([[1.0], [2.0], [3.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = gaussian_exp.apply(gaussian_weight)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = gaussian_weight.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    gaussian_weight.grad.zero_()\n",
    "    output = autograd_test(gaussian_weight)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = gaussian_weight.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"My grads do not match autograd grads (multiple dimensions)!\"\n",
    "    print(\"My grads match autograd grads (multiple dimensions).\")\n",
    "\n",
    "    # 4. Edge case: Very large and very small values\n",
    "    gaussian_weight = torch.tensor([[1e-5], [1e5]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = gaussian_exp.apply(gaussian_weight)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = gaussian_weight.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    gaussian_weight.grad.zero_()\n",
    "    output = autograd_test(gaussian_weight)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = gaussian_weight.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"My grads do not match autograd grads (edge cases)!\"\n",
    "    print(\"My grads match autograd grads (edge cases).\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad inv cov:  torch.Size([1, 2, 2])\n",
      "Grad output:  torch.Size([1, 1])\n",
      "Diff:  torch.Size([1, 1, 2])\n",
      "Grad inv cov:  torch.Size([1, 2, 2])\n",
      "Grad output:  torch.Size([1, 1])\n",
      "Diff:  torch.Size([1, 1, 2])\n",
      "Grad inv cov:  torch.Size([1, 2, 2])\n",
      "Grad output:  torch.Size([1, 1])\n",
      "Diff:  torch.Size([1, 1, 2])\n",
      "Grad inv cov:  torch.Size([1, 2, 2])\n",
      "Grad output:  torch.Size([1, 1])\n",
      "Diff:  torch.Size([1, 1, 2])\n",
      "Gradcheck passed.\n",
      "Grad inv cov:  torch.Size([1, 2, 2])\n",
      "Grad output:  torch.Size([1, 1])\n",
      "Diff:  torch.Size([1, 1, 2])\n",
      "Gradients match for single pixel.\n",
      "Grad inv cov:  torch.Size([2, 2, 2])\n",
      "Grad output:  torch.Size([2, 1])\n",
      "Diff:  torch.Size([2, 1, 2])\n",
      "Output:  tensor([[-7.0000],\n",
      "        [-0.4375]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "My grads:  tensor([[ 4.0000,  3.0000],\n",
      "        [-1.0000, -0.7500]], dtype=torch.float64)\n",
      "Autograd grads:  tensor([[ 4.0000,  3.0000],\n",
      "        [-1.0000, -0.7500]], dtype=torch.float64)\n",
      "Gradients match for multiple pixels.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "\n",
    "class gaussian_weight(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, gaussian_mean: torch.Tensor, inverted_covariance: torch.Tensor, pixel: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Pixel means are a nx2 tensor, inverted covariance is a 2x2 tensor, pixel is a nx2 tensor\n",
    "        Outputs a nx1 tensor\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(gaussian_mean, inverted_covariance, pixel)\n",
    "        diff = (pixel - gaussian_mean).unsqueeze(1)\n",
    "        # 2x2 * 2x1 = 2x1\n",
    "        inv_cov_mult = torch.einsum('bij,bjk->bik', inverted_covariance, diff.transpose(1, 2))\n",
    "        return -0.5 * torch.einsum('bij,bjk->bik', diff, inv_cov_mult).squeeze(-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        \"\"\"Output of forward is a nx1 tensor so the grad_output is a nx1 tensor\"\"\"\n",
    "        gaussian_mean, inverted_covariance, pixel = ctx.saved_tensors\n",
    "        diff = (pixel - gaussian_mean).unsqueeze(1)  # nx2x1\n",
    "\n",
    "        deriv_wrt_inv_cov = -0.5 * torch.einsum(\"bij,bjk->bik\", diff.transpose(1, 2), diff)\n",
    "        grad_inv_cov = grad_output * deriv_wrt_inv_cov  # output is nx2x2\n",
    "        print(\"Grad inv cov: \", grad_inv_cov.shape)\n",
    "        print(\"Grad output: \", grad_output.shape)\n",
    "        print(\"Diff: \", diff.shape)\n",
    "\n",
    "        # deriv_wrt_diff = -0.5 * 2 * torch.einsum(\"bij,bjk->bik\", diff, inverted_covariance)\n",
    "        # deriv_wrt_gaussian_mean = -1\n",
    "        # grad_gaussian_mean = torch.einsum(\"bi,bij->bj\", grad_output, deriv_wrt_diff) * deriv_wrt_gaussian_mean]\n",
    "        deriv_output_wrt_diff1 = torch.einsum(\"bij,bjk->bik\", inverted_covariance, diff.transpose(1, 2))\n",
    "        deriv_output_wrt_diff2 = torch.einsum(\"bij,bjk->bik\", inverted_covariance.transpose(1, 2), diff.transpose(1, 2))\n",
    "        deriv_output_wrt_diff = -0.5 * torch.einsum(\"bi,bji->bj\", grad_output, deriv_output_wrt_diff1 + deriv_output_wrt_diff2)\n",
    "        grad_gaussian_mean = deriv_output_wrt_diff * -1\n",
    "        return grad_gaussian_mean, grad_inv_cov, None\n",
    "    \n",
    "\n",
    "# Define a test function using PyTorch's autograd\n",
    "def autograd_test(gaussian_mean, inverted_covariance, pixel):\n",
    "    diff = (pixel - gaussian_mean).unsqueeze(1)\n",
    "    # 2x2 * 2x1 = 2x1\n",
    "    inv_cov_mult = torch.einsum('bij,bjk->bik', inverted_covariance, diff.transpose(1, 2))\n",
    "    return -0.5 * torch.einsum('bij,bjk->bik', diff, inv_cov_mult).squeeze(-1)\n",
    "\n",
    "# Helper function to compare gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# Test cases\n",
    "def run_tests():\n",
    "    tolerance = 1e-6\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    gaussian_mean = torch.tensor([[1.0, 2.0]], requires_grad=True, dtype=torch.float64)\n",
    "    inverted_covariance = torch.tensor([[[2.0, 0.0], [0.0, 1.5]]], requires_grad=True, dtype=torch.float64)\n",
    "    pixel = torch.tensor([[1.5, 2.5]], requires_grad=False, dtype=torch.float64)\n",
    "\n",
    "    assert gradcheck(gaussian_weight.apply, (gaussian_mean, inverted_covariance, pixel)), \"Gradcheck failed!\"\n",
    "    print(\"Gradcheck passed.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd (single pixel)\n",
    "    gaussian_mean = torch.tensor([[1.0, 2.0]], requires_grad=True, dtype=torch.float64)\n",
    "    inverted_covariance = torch.tensor([[[2.0, 0.0], [0.0, 1.5]]], requires_grad=True, dtype=torch.float64)\n",
    "    pixel = torch.tensor([[1.5, 2.5]], requires_grad=False, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = gaussian_weight.apply(gaussian_mean, inverted_covariance, pixel)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grad_gaussian_mean = gaussian_mean.grad.clone()\n",
    "    my_grad_inv_cov = inverted_covariance.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    gaussian_mean.grad.zero_()\n",
    "    inverted_covariance.grad.zero_()\n",
    "    output = autograd_test(gaussian_mean, inverted_covariance, pixel)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grad_gaussian_mean = gaussian_mean.grad.clone()\n",
    "    autograd_grad_inv_cov = inverted_covariance.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grad_gaussian_mean, autograd_grad_gaussian_mean, tolerance), \"Mismatch in grads (gaussian_mean, single pixel)!\"\n",
    "    assert compare_grads(my_grad_inv_cov, autograd_grad_inv_cov, tolerance), \"Mismatch in grads (inverted_covariance, single pixel)!\"\n",
    "    print(\"Gradients match for single pixel.\")\n",
    "\n",
    "    # 3. Test with multiple pixels\n",
    "    gaussian_mean = torch.tensor([[1.0, 2.0], [2.0, 3.0]], requires_grad=True, dtype=torch.float64)\n",
    "    inverted_covariance = torch.tensor([[[2.0, 0.0], [0.0, 1.5]]], requires_grad=True, dtype=torch.float64)\n",
    "    pixel = torch.tensor([[3.0, 4.0], [1.5, 2.5]], requires_grad=False, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = gaussian_weight.apply(gaussian_mean, inverted_covariance, pixel)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grad_gaussian_mean = gaussian_mean.grad.clone()\n",
    "    my_grad_inv_cov = inverted_covariance.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    gaussian_mean.grad.zero_()\n",
    "    inverted_covariance.grad.zero_()\n",
    "    output = autograd_test(gaussian_mean, inverted_covariance, pixel)\n",
    "    print(\"Output: \", output)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grad_gaussian_mean = gaussian_mean.grad.clone()\n",
    "    autograd_grad_inv_cov = inverted_covariance.grad.clone()\n",
    "\n",
    "    print(\"My grads: \", my_grad_gaussian_mean)\n",
    "    print(\"Autograd grads: \", autograd_grad_gaussian_mean)\n",
    "    assert compare_grads(my_grad_gaussian_mean, autograd_grad_gaussian_mean, tolerance), \"Mismatch in grads (gaussian_mean, multiple pixels)!\"\n",
    "    assert compare_grads(my_grad_inv_cov, autograd_grad_inv_cov, tolerance), \"Mismatch in grads (inverted_covariance, multiple pixels)!\"\n",
    "    print(\"Gradients match for multiple pixels.\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed.\n",
      "Gradients match for single matrix.\n",
      "Gradients match for multiple matrices.\n",
      "Correctly raised error for non-invertible matrix: Infinite values in final matrices\n",
      "Gradients match for larger batch size.\n"
     ]
    }
   ],
   "source": [
    "def d_inv_wrt_a(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, d: torch.Tensor, grad_output: torch.Tensor):\n",
    "    \"\"\"\n",
    "    All tensors are nx1 tensors - returns a nx1 tensor by summing over the last dimension\n",
    "    \"\"\"\n",
    "    det = a * d - b * c\n",
    "    deriv = -1 * (d**2) / (det**2) * grad_output[:, 0, 0]\n",
    "    deriv += (b*d) / (det**2) * grad_output[:, 0, 1]\n",
    "    deriv += (c*d) / (det**2) * grad_output[:, 1, 0]\n",
    "    deriv += -1 * (b*c) / (det**2) * grad_output[:, 1, 1]\n",
    "    return deriv\n",
    "\n",
    "def d_inv_wrt_b(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, d: torch.Tensor, grad_output: torch.Tensor):                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "    det = a * d - b * c\n",
    "    deriv = (c * d) / (det**2) * grad_output[:, 0, 0]\n",
    "    deriv += -1 * (a * d) / (det**2) * grad_output[:, 0, 1]\n",
    "    deriv += -1 * (c*c) / (det**2) * grad_output[:, 1, 0]\n",
    "    deriv += (a*c) / (det**2) * grad_output[:, 1, 1]\n",
    "    return deriv\n",
    "\n",
    "def d_inv_wrt_c(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, d: torch.Tensor, grad_output: torch.Tensor):\n",
    "    det = a * d - b * c\n",
    "    deriv = (b*d) / (det**2) * grad_output[:, 0, 0]\n",
    "    deriv += -1 * (b*b) / (det**2) * grad_output[:, 0, 1]\n",
    "    deriv += -1 * (a*d) / (det**2) * grad_output[:, 1, 0]\n",
    "    deriv += (a*b) / (det**2) * grad_output[:, 1, 1]\n",
    "    return deriv\n",
    "\n",
    "def d_inv_wrt_d(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, d: torch.Tensor, grad_output: torch.Tensor):\n",
    "    det = a * d - b * c\n",
    "    deriv = -1 * (b*c) / (det**2) * grad_output[:, 0, 0]\n",
    "    deriv += (a*b) / (det**2) * grad_output[:, 0, 1]\n",
    "    deriv += (a*c) / (det**2) * grad_output[:, 1, 0]\n",
    "    deriv += -1 * (a*a) / (det**2) * grad_output[:, 1, 1]\n",
    "    return deriv\n",
    "\n",
    "class invert_2x2_matrix(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, matrix: torch.Tensor):\n",
    "        \"\"\"input is a nx2x2 tensor, returns the inverse of each 2x2 matrix\"\"\"\n",
    "        ctx.save_for_backward(matrix)\n",
    "        det = matrix[:, 0, 0] * matrix[:, 1, 1] - matrix[:, 0, 1] * matrix[:, 1, 0]\n",
    "        # Create empty nx2x2 tensor for the inverted matrices\n",
    "        final_matrices = torch.zeros_like(matrix)\n",
    "        \n",
    "        # Fill in the inverted matrix elements using the 2x2 matrix inverse formula\n",
    "        final_matrices[:, 0, 0] = matrix[:, 1, 1] / det  \n",
    "        final_matrices[:, 0, 1] = -matrix[:, 0, 1] / det\n",
    "        final_matrices[:, 1, 0] = -matrix[:, 1, 0] / det\n",
    "        final_matrices[:, 1, 1] = matrix[:, 0, 0] / det\n",
    "\n",
    "        if torch.isinf(final_matrices).any():\n",
    "            raise RuntimeError(\"Infinite values in final matrices\")\n",
    "        return final_matrices\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        \"\"\"grad_output is a nx2x2 tensor, returns the gradient of the inverse of each 2x2 matrix\"\"\"\n",
    "        matrix = ctx.saved_tensors[0]\n",
    "        a = matrix[:, 0, 0]\n",
    "        b = matrix[:, 0, 1]\n",
    "        c = matrix[:, 1, 0]\n",
    "        d = matrix[:, 1, 1]\n",
    "        grad_a = d_inv_wrt_a(a, b, c, d, grad_output)\n",
    "        grad_b = d_inv_wrt_b(a, b, c, d, grad_output)\n",
    "        grad_c = d_inv_wrt_c(a, b, c, d, grad_output)\n",
    "        grad_d = d_inv_wrt_d(a, b, c, d, grad_output)\n",
    "        return torch.stack([grad_a, grad_b, grad_c, grad_d], dim=-1).view(matrix.shape)\n",
    "    \n",
    "def invert_2x2_matrix_test(matrix: torch.Tensor):\n",
    "    det = matrix[:, 0, 0] * matrix[:, 1, 1] - matrix[:, 0, 1] * matrix[:, 1, 0]\n",
    "    # Create empty nx2x2 tensor for the inverted matrices\n",
    "    final_matrices = torch.zeros_like(matrix)\n",
    "    \n",
    "    # Fill in the inverted matrix elements using the 2x2 matrix inverse formula\n",
    "    final_matrices[:, 0, 0] = matrix[:, 1, 1] / det  \n",
    "    final_matrices[:, 0, 1] = -matrix[:, 0, 1] / det\n",
    "    final_matrices[:, 1, 0] = -matrix[:, 1, 0] / det\n",
    "    final_matrices[:, 1, 1] = matrix[:, 0, 0] / det\n",
    "    return final_matrices\n",
    "\n",
    "\n",
    "# Define helper functions to compare gradients and compute reference gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# Test cases for invert_2x2_matrix\n",
    "def run_tests():\n",
    "    tolerance = 1e-6\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    matrix = torch.tensor(\n",
    "        [[[4.0, 7.0], [2.0, 6.0]]], requires_grad=True, dtype=torch.float64\n",
    "    )  # Single 2x2 matrix\n",
    "    assert gradcheck(invert_2x2_matrix.apply, (matrix,)), \"Gradcheck failed!\"\n",
    "    print(\"Gradcheck passed.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd (single matrix)\n",
    "    matrix = torch.tensor(\n",
    "        [[[4.0, 7.0], [2.0, 6.0]]], requires_grad=True, dtype=torch.float64\n",
    "    )\n",
    "    # My grads\n",
    "    output = invert_2x2_matrix.apply(matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = matrix.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    matrix.grad.zero_()\n",
    "    output = invert_2x2_matrix_test(matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = matrix.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"Mismatch in gradients (single matrix)!\"\n",
    "    print(\"Gradients match for single matrix.\")\n",
    "\n",
    "    # 3. Test with multiple matrices\n",
    "    matrix = torch.tensor(\n",
    "        [[[4.0, 7.0], [2.0, 6.0]], [[1.0, 2.0], [3.0, 4.0]]], requires_grad=True, dtype=torch.float64\n",
    "    )\n",
    "    # My grads\n",
    "    output = invert_2x2_matrix.apply(matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = matrix.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    matrix.grad.zero_()\n",
    "    output = invert_2x2_matrix_test(matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = matrix.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"Mismatch in gradients (multiple matrices)!\"\n",
    "    print(\"Gradients match for multiple matrices.\")\n",
    "\n",
    "    # 4. Edge cases: non-invertible matrix\n",
    "    try:\n",
    "        matrix = torch.tensor(\n",
    "            [[[1.0, 2.0], [2.0, 4.0]]], requires_grad=True, dtype=torch.float64\n",
    "        )\n",
    "        output = invert_2x2_matrix.apply(matrix)\n",
    "        print(\"Output: \", output)\n",
    "        print(\"Non-invertible matrix did not raise an error as expected.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Correctly raised error for non-invertible matrix: {e}\")\n",
    "\n",
    "    # 5. Test with larger batch sizes\n",
    "    batch_size = 10\n",
    "    matrix = torch.rand((batch_size, 2, 2), requires_grad=True, dtype=torch.float64)\n",
    "    # My grads\n",
    "    output = invert_2x2_matrix.apply(matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = matrix.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    matrix.grad.zero_()\n",
    "    output = invert_2x2_matrix_test(matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = matrix.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"Mismatch in gradients (batch size 10)!\"\n",
    "    print(\"Gradients match for larger batch size.\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U grad tensor([[[-2.6232, -2.6232, -2.6232],\n",
      "         [ 1.6089,  1.6089,  1.6089],\n",
      "         [ 2.0174,  2.0174,  2.0174]]], dtype=torch.float64)\n",
      "output tensor([[[-0.3142,  0.7716, -0.6829],\n",
      "         [ 1.5104, -1.4269,  1.5671],\n",
      "         [-1.0433,  1.1982, -1.2437]]], dtype=torch.float64,\n",
      "       grad_fn=<BmmBackward0>)\n",
      "U grad tensor([[[-2.6232, -2.6232, -2.6232],\n",
      "         [ 1.6089,  1.6089,  1.6089],\n",
      "         [ 2.0174,  2.0174,  2.0174]]], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class covariance_3d_to_covariance_2d(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, covariance_3d: torch.Tensor, U: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Covariance 3d is the nx3x3 covariance matrix.\n",
    "        U is the J@W.T matrix. this is a 3x3 matrix\n",
    "        To get the covariance 2d we do U.T @ covariance_3d @ U\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(U, covariance_3d)\n",
    "        outcome = torch.bmm(U.transpose(1, 2), torch.bmm(covariance_3d, U))\n",
    "        return outcome\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        U, covariance_3d = ctx.saved_tensors\n",
    "\n",
    "        # Derivative of (U^T * C * U) w.r.t. C = U * grad_output * U^T\n",
    "        # grad_cov3d = torch.einsum(\"nij,njk->nik\", U, grad_output)\n",
    "        # grad_cov3d = torch.einsum(\"nij,njk->nik\", grad_cov3d, U.transpose(1, 2))\n",
    "        grad_cov3d = U @ grad_output @ U.transpose(1, 2)\n",
    "        # Derivative of (U^T * C * U) w.r.t. U\n",
    "        # Z = (U^T * (C * U)) Y= C * U\n",
    "        # the contribution from Y is covariance_3d.T @ grad_output\n",
    "        y = torch.einsum(\"nij,njk->nik\", covariance_3d, U)\n",
    "        deriv_U_first_part = torch.einsum(\"nij,njk->nik\", grad_output, y.transpose(1, 2)).transpose(1, 2)\n",
    "        dz_dy = torch.einsum(\"nij,njk->nik\", U, grad_output)\n",
    "        dy_du = torch.einsum(\"nij,njk->nik\", covariance_3d.transpose(1, 2), dz_dy)\n",
    "        return grad_cov3d, deriv_U_first_part + dy_du\n",
    "\n",
    "def covariance_3d_to_covariance_2d_test(E: torch.Tensor, U: torch.Tensor):\n",
    "    \"\"\"E is a nx3x3 matrix, U is a n3x3 matrix\"\"\"\n",
    "    return torch.bmm(U.transpose(1, 2), torch.bmm(covariance_3d, U))\n",
    "\n",
    "# Define helper functions for comparing gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "covariance_3d = torch.randn(1, 3, 3, requires_grad=True, dtype=torch.float64)\n",
    "U = torch.randn(1, 3, 3, requires_grad=True, dtype=torch.float64)\n",
    "# print(\"U\", U)\n",
    "\n",
    "output = covariance_3d_to_covariance_2d.apply(covariance_3d, U)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "# print(\"output\", output)\n",
    "# print(\"cov grad\", covariance_3d.grad)\n",
    "print(\"U grad\", U.grad)\n",
    "\n",
    "# auto test\n",
    "torch.random.manual_seed(0)\n",
    "covariance_3d = torch.randn(1, 3, 3, requires_grad=True, dtype=torch.float64)\n",
    "U = torch.randn(1, 3, 3, requires_grad=True, dtype=torch.float64)\n",
    "# print(\"U\", U)\n",
    "\n",
    "output = covariance_3d_to_covariance_2d_test(covariance_3d, U)\n",
    "print(\"output\", output)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "# print(\"cov grad\", covariance_3d.grad)\n",
    "print(\"U grad\", U.grad)\n",
    "\n",
    "gradcheck(covariance_3d_to_covariance_2d.apply, (covariance_3d, U))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed.\n",
      "R.grad: tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]], dtype=torch.float64), S.grad: tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]], dtype=torch.float64)\n",
      "Gradients match for single matrix.\n",
      "Gradients match for multiple matrices.\n",
      "Output:  tensor([[[0.9408, 0.7610, 0.4686],\n",
      "         [0.9541, 0.6338, 0.1669],\n",
      "         [0.8051, 0.8766, 0.8510]]], dtype=torch.float64,\n",
      "       grad_fn=<R_S_To_MBackward>)\n",
      "Non-orthogonal R or S did not raise an error as expected.\n",
      "Gradients match for larger batch size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s3/bqcdtp2s6652tsxqm4hhv4gr0000gn/T/ipykernel_56530/3943065171.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R = torch.tensor(c, requires_grad=True, dtype=torch.float64)\n",
      "/var/folders/s3/bqcdtp2s6652tsxqm4hhv4gr0000gn/T/ipykernel_56530/3943065171.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  S = torch.tensor(c, requires_grad=True, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "class R_S_To_M(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, R: torch.Tensor, S: torch.Tensor):\n",
    "        \"\"\"R is a nx3x3 rotation matrix, S is a nx3x3 scale matrix\"\"\"\n",
    "        ctx.save_for_backward(R, S)\n",
    "        return torch.einsum(\"nij,njk->nik\", R, S)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        R, S = ctx.saved_tensors\n",
    "        grad_R = torch.einsum(\"nij,njk->nik\", grad_output, S.transpose(1, 2))\n",
    "        grad_S = torch.einsum(\"nij,njk->nik\", R.transpose(1, 2), grad_output)\n",
    "        return grad_R, grad_S\n",
    "\n",
    "# Define helper functions for comparing gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# Test cases for R_S_To_M\n",
    "def run_R_S_To_M_tests():\n",
    "    tolerance = 1e-6\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness3, 3, requires_grad=True, dtype=torch.float64)\n",
    "    R = torch.randn((1, 3, 3), requires_grad=True, dtype=torch.float64)\n",
    "    S = torch.randn((1, 3, 3), requires_grad=True, dtype=torch.float64)\n",
    "    assert gradcheck(R_S_To_M.apply, (R, S)), \"Gradcheck failed!\"\n",
    "    print(\"Gradcheck passed.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd (single matrix)\n",
    "    c = torch.eye(3, requires_grad=False, dtype=torch.float64).unsqueeze(0)\n",
    "    R = torch.tensor(c, requires_grad=True, dtype=torch.float64)\n",
    "    S = torch.tensor(c, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = R_S_To_M.apply(R, S)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    print(f\"R.grad: {R.grad}, S.grad: {S.grad}\")\n",
    "    my_grads_R = R.grad.clone()\n",
    "    my_grads_S = S.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    R.grad.zero_()\n",
    "    S.grad.zero_()\n",
    "    output = torch.einsum(\"nij,njk->nik\", R, S)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_R = R.grad.clone()\n",
    "    autograd_grads_S = S.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_R, autograd_grads_R, tolerance), \"Mismatch in gradients for R (single matrix)!\"\n",
    "    assert compare_grads(my_grads_S, autograd_grads_S, tolerance), \"Mismatch in gradients for S (single matrix)!\"\n",
    "    print(\"Gradients match for single matrix.\")\n",
    "\n",
    "    # 3. Test with multiple matrices\n",
    "    R = torch.stack([torch.randn((3, 3), dtype=torch.float64) for _ in range(5)], dim=0).requires_grad_()\n",
    "    S = torch.stack([torch.randn((3, 3), dtype=torch.float64) for _ in range(5)], dim=0).requires_grad_()\n",
    "\n",
    "    # My grads\n",
    "    output = R_S_To_M.apply(R, S)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_R = R.grad.clone()\n",
    "    my_grads_S = S.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    R.grad.zero_()\n",
    "    S.grad.zero_()\n",
    "    output = torch.einsum(\"nij,njk->nik\", R, S)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_R = R.grad.clone()\n",
    "    autograd_grads_S = S.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_R, autograd_grads_R, tolerance), \"Mismatch in gradients for R (multiple matrices)!\"\n",
    "    assert compare_grads(my_grads_S, autograd_grads_S, tolerance), \"Mismatch in gradients for S (multiple matrices)!\"\n",
    "    print(\"Gradients match for multiple matrices.\")\n",
    "\n",
    "    # 4. Edge cases: non-orthogonal R or S\n",
    "    try:\n",
    "        R = torch.rand((1, 3, 3), requires_grad=True, dtype=torch.float64)\n",
    "        S = torch.rand((1, 3, 3), requires_grad=True, dtype=torch.float64)\n",
    "        output = R_S_To_M.apply(R, S)\n",
    "        print(\"Output: \", output)\n",
    "        print(\"Non-orthogonal R or S did not raise an error as expected.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Handled non-orthogonal case correctly: {e}\")\n",
    "\n",
    "    # 5. Test with larger batch sizes\n",
    "    batch_size = 10\n",
    "    R = torch.stack([torch.eye(3, dtype=torch.float64) for _ in range(batch_size)], dim=0).requires_grad_()\n",
    "    S = torch.stack([torch.eye(3, dtype=torch.float64) for _ in range(batch_size)], dim=0).requires_grad_()\n",
    "\n",
    "    # My grads\n",
    "    output = R_S_To_M.apply(R, S)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_R = R.grad.clone()\n",
    "    my_grads_S = S.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    R.grad.zero_()\n",
    "    S.grad.zero_()\n",
    "    output = torch.einsum(\"nij,njk->nik\", R, S)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_R = R.grad.clone()\n",
    "    autograd_grads_S = S.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_R, autograd_grads_R, tolerance), \"Mismatch in gradients for R (batch size 10)!\"\n",
    "    assert compare_grads(my_grads_S, autograd_grads_S, tolerance), \"Mismatch in gradients for S (batch size 10)!\"\n",
    "    print(\"Gradients match for larger batch size.\")\n",
    "\n",
    "# Run the tests\n",
    "run_R_S_To_M_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed.\n",
      "M.grad: tensor([[[ 2.,  4.,  6.],\n",
      "         [ 8., 10., 12.],\n",
      "         [14., 16., 18.]]], dtype=torch.float64)\n",
      "Gradients match for single matrix.\n",
      "Gradients match for multiple matrices.\n",
      "Gradients match for zero matrix.\n",
      "Gradients match for larger batch size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s3/bqcdtp2s6652tsxqm4hhv4gr0000gn/T/ipykernel_56530/4142878649.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  M = torch.tensor(M.unsqueeze(0), requires_grad=True, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "class M_to_covariance(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, M: torch.Tensor):\n",
    "        ctx.save_for_backward(M)\n",
    "        return M.pow(2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        M = ctx.saved_tensors[0]\n",
    "        return 2 * grad_output * M\n",
    "    \n",
    "def test_M_To_Covariance(M: torch.Tensor):\n",
    "    return M.pow(2)\n",
    "\n",
    "# Helper functions for testing\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# Test cases for M_To_Covariance\n",
    "def run_M_To_Covariance_tests():\n",
    "    tolerance = 1e-6\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    M = torch.randn((2, 3, 3), requires_grad=True, dtype=torch.float64)\n",
    "    assert gradcheck(M_to_covariance.apply, (M,)), \"Gradcheck failed!\"\n",
    "    print(\"Gradcheck passed.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd (single matrix)\n",
    "    M = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],\n",
    "                     requires_grad=True, dtype=torch.float64)\n",
    "    M = torch.tensor(M.unsqueeze(0), requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = M_to_covariance.apply(M)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    print(f\"M.grad: {M.grad}\")\n",
    "    my_grads_M = M.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    M.grad.zero_()\n",
    "    output = test_M_To_Covariance(M)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_M = M.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_M, autograd_grads_M, tolerance), \"Mismatch in gradients for M (single matrix)!\"\n",
    "    print(\"Gradients match for single matrix.\")\n",
    "\n",
    "    # 3. Test with multiple matrices\n",
    "    M = torch.stack([torch.randn((3, 3), dtype=torch.float64) for _ in range(5)], dim=0).requires_grad_()\n",
    "\n",
    "    # My grads\n",
    "    output = M_to_covariance.apply(M)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_M = M.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    M.grad.zero_()\n",
    "    output = M.pow(2)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_M = M.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_M, autograd_grads_M, tolerance), \"Mismatch in gradients for M (multiple matrices)!\"\n",
    "    print(\"Gradients match for multiple matrices.\")\n",
    "\n",
    "    # 4. Test edge cases: matrix with zeros\n",
    "    M = torch.zeros((1, 3, 3), requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = M_to_covariance.apply(M)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_M = M.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    M.grad.zero_()\n",
    "    output = test_M_To_Covariance(M)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_M = M.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_M, autograd_grads_M, tolerance), \"Mismatch in gradients for M (zero matrix)!\"\n",
    "    print(\"Gradients match for zero matrix.\")\n",
    "\n",
    "    # 5. Test with larger batch sizes\n",
    "    batch_size = 10\n",
    "    M = torch.randn((batch_size, 3, 3), requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = M_to_covariance.apply(M)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_M = M.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    M.grad.zero_()\n",
    "    output = test_M_To_Covariance(M)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_M = M.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_M, autograd_grads_M, tolerance), \"Mismatch in gradients for M (batch size 10)!\"\n",
    "    print(\"Gradients match for larger batch size.\")\n",
    "\n",
    "# Run the tests\n",
    "run_M_To_Covariance_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed.\n",
      "Quats.grad: tensor([[0., 0., 0., 0.]], dtype=torch.float64)\n",
      "Gradients match for single quaternion.\n",
      "Gradients match for multiple quaternions.\n",
      "Gradients match for zero quaternion.\n",
      "Gradients match for larger batch size.\n",
      "Normalization test passed.\n"
     ]
    }
   ],
   "source": [
    "from splat.utils import build_rotation\n",
    "\n",
    "def d_r_wrt_qr(quats: torch.Tensor, n: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the derivative of m wrt quats\n",
    "    quats is nx4 tensor\n",
    "    shape is nx3 tensor\n",
    "    \"\"\"\n",
    "    qr = quats[:, 0]\n",
    "    qi = quats[:, 1]\n",
    "    qj = quats[:, 2]\n",
    "    qk = quats[:, 3]\n",
    "\n",
    "    derivative = torch.zeros((n, 3, 3))\n",
    "    derivative[:, 0, 1] = -qk\n",
    "    derivative[:, 0, 2] = qj\n",
    "    derivative[:, 1, 0] = qk\n",
    "    derivative[:, 1, 2] = -qi\n",
    "    derivative[:, 2, 0] = -qj\n",
    "    derivative[:, 2, 1] = qi\n",
    "\n",
    "    return 2 * derivative\n",
    "\n",
    "\n",
    "def d_r_wrt_qi(quats: torch.Tensor, n: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the derivative of m wrt quats\n",
    "    quats is nx4 tensor\n",
    "    shape is nx3 tensor\n",
    "    \"\"\"\n",
    "    qr = quats[:, 0]\n",
    "    qi = quats[:, 1]\n",
    "    qj = quats[:, 2]\n",
    "    qk = quats[:, 3]\n",
    "\n",
    "    derivative = torch.zeros((n, 3, 3))\n",
    "    derivative[:, 0, 1] = qj\n",
    "    derivative[:, 0, 2] = qk\n",
    "    derivative[:, 1, 0] = qj\n",
    "    derivative[:, 1, 1] = -2 * qi\n",
    "    derivative[:, 1, 2] = -qr\n",
    "    derivative[:, 2, 0] = qk\n",
    "    derivative[:, 2, 1] = qr\n",
    "    derivative[:, 2, 2] = -2 * qi\n",
    "    return 2 * derivative\n",
    "\n",
    "\n",
    "def d_r_wrt_qj(quats: torch.Tensor, n: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the derivative of m wrt quats\n",
    "    quats is nx4 tensor\n",
    "    shape is nx3 tensor\n",
    "    \"\"\"\n",
    "    qr = quats[:, 0]\n",
    "    qi = quats[:, 1]\n",
    "    qj = quats[:, 2]\n",
    "    qk = quats[:, 3]\n",
    "\n",
    "    derivative = torch.zeros((n, 3, 3))\n",
    "    derivative[:, 0, 0] = -2 * qj\n",
    "    derivative[:, 0, 1] = qi\n",
    "    derivative[:, 0, 2] = qr\n",
    "    derivative[:, 1, 0] = qi\n",
    "    derivative[:, 1, 1] = 0\n",
    "    derivative[:, 1, 2] = qk\n",
    "    derivative[:, 2, 0] = -qr\n",
    "    derivative[:, 2, 1] = qk\n",
    "    derivative[:, 2, 2] = -2 * qj\n",
    "    return 2 * derivative\n",
    "\n",
    "\n",
    "def d_r_wrt_qk(quats: torch.Tensor, n: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the derivative of m wrt quats\n",
    "    quats is nx4 tensor\n",
    "    shape is nx3 tensor\n",
    "    \"\"\"\n",
    "    qr = quats[:, 0]\n",
    "    qi = quats[:, 1]\n",
    "    qj = quats[:, 2]\n",
    "    qk = quats[:, 3]\n",
    "\n",
    "    derivative = torch.zeros((n, 3, 3))\n",
    "    derivative[:, 0, 0] = -2 * qk\n",
    "    derivative[:, 0, 1] = -qr\n",
    "    derivative[:, 0, 2] = qi\n",
    "    derivative[:, 1, 0] = qr\n",
    "    derivative[:, 1, 1] = -2*qk\n",
    "    derivative[:, 1, 2] = qj\n",
    "    derivative[:, 2, 0] = qi\n",
    "    derivative[:, 2, 1] = qj\n",
    "    derivative[:, 2, 2] = 0\n",
    "    return 2 * derivative\n",
    "\n",
    "class quats_to_R(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, quats: torch.Tensor) -> torch.Tensor:\n",
    "        ctx.save_for_backward(quats)\n",
    "        R = build_rotation(quats, normalize=False)\n",
    "        return R\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        grad output is nx3x3 and the jacobian nx3x3x4 so deriv_wrt_qr \n",
    "        is nx3x3 where each entry is the derivative wrt one element in the final wrt r\n",
    "        \"\"\"\n",
    "        quats = ctx.saved_tensors[0]\n",
    "        deriv_wrt_qr = d_r_wrt_qr(quats, quats.shape[0])\n",
    "        deriv_wrt_qi = d_r_wrt_qi(quats, quats.shape[0])\n",
    "        deriv_wrt_qj = d_r_wrt_qj(quats, quats.shape[0])\n",
    "        deriv_wrt_qk = d_r_wrt_qk(quats, quats.shape[0])\n",
    "        \n",
    "        deriv_wrt_qr = (grad_output * deriv_wrt_qr).sum(dim=(1, 2), keepdim=True).squeeze(2)\n",
    "        deriv_wrt_qi = (grad_output * deriv_wrt_qi).sum(dim=(1, 2), keepdim=True).squeeze(2)\n",
    "        deriv_wrt_qj = (grad_output * deriv_wrt_qj).sum(dim=(1, 2), keepdim=True).squeeze(2)\n",
    "        deriv_wrt_qk = (grad_output * deriv_wrt_qk).sum(dim=(1, 2), keepdim=True).squeeze(2)\n",
    "        return torch.cat([deriv_wrt_qr, deriv_wrt_qi, deriv_wrt_qj, deriv_wrt_qk], dim=1)\n",
    "    \n",
    "def test_quats_to_R(quats: torch.Tensor):\n",
    "    return build_rotation(quats, normalize=False)\n",
    "\n",
    "\n",
    "# Helper function to compare gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# Test suite for quats_to_R\n",
    "def run_quats_to_R_tests():\n",
    "    tolerance = 1e-6\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    quats = torch.randn((5, 4), requires_grad=True, dtype=torch.float64)\n",
    "    assert gradcheck(quats_to_R.apply, (quats,)), \"Gradcheck failed!\"\n",
    "    print(\"Gradcheck passed.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd (single quaternion)\n",
    "    quats = torch.tensor([[1.0, 0.0, 0.0, 0.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = quats_to_R.apply(quats)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    print(f\"Quats.grad: {quats.grad}\")\n",
    "    my_grads = quats.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    quats.grad.zero_()\n",
    "    output = build_rotation(quats, normalize=False)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = quats.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"Mismatch in gradients for single quaternion!\"\n",
    "    print(\"Gradients match for single quaternion.\")\n",
    "\n",
    "    # 3. Test with multiple quaternions\n",
    "    quats = torch.randn((5, 4), dtype=torch.float64).requires_grad_()\n",
    "\n",
    "    # My grads\n",
    "    output = quats_to_R.apply(quats)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = quats.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    quats.grad.zero_()\n",
    "    output = build_rotation(quats, normalize=False)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = quats.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"Mismatch in gradients for multiple quaternions!\"\n",
    "    print(\"Gradients match for multiple quaternions.\")\n",
    "\n",
    "    # 4. Test edge cases: zero quaternion\n",
    "    quats = torch.zeros((1, 4), requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = quats_to_R.apply(quats)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = quats.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    quats.grad.zero_()\n",
    "    output = build_rotation(quats, normalize=False)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = quats.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"Mismatch in gradients for zero quaternion!\"\n",
    "    print(\"Gradients match for zero quaternion.\")\n",
    "\n",
    "    # 5. Test with larger batch sizes\n",
    "    batch_size = 10\n",
    "    quats = torch.randn((batch_size, 4), requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = quats_to_R.apply(quats)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = quats.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    quats.grad.zero_()\n",
    "    output = build_rotation(quats, normalize=False)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = quats.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"Mismatch in gradients for larger batch size!\"\n",
    "    print(\"Gradients match for larger batch size.\")\n",
    "    print(\"Normalization test passed.\")\n",
    "\n",
    "# Run the tests\n",
    "run_quats_to_R_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed.\n",
      "My grads match autograd grads (single quaternion).\n",
      "My grads match autograd grads (multiple quaternions).\n",
      "My grads match autograd grads (edge cases).\n"
     ]
    }
   ],
   "source": [
    "class normalize_quats(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, quats: torch.Tensor):\n",
    "        \"\"\"Quats are a nx4 tensor\"\"\"\n",
    "        ctx.save_for_backward(quats)\n",
    "        return quats / quats.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        quats = ctx.saved_tensors[0]\n",
    "        norm = quats.norm(dim=1, keepdim=True)\n",
    "        norm_cube = norm ** 3\n",
    "\n",
    "        quats_outer = torch.einsum('ni,nj->nij', quats, quats)\n",
    "        eye = torch.eye(4, dtype=quats.dtype, device=quats.device).unsqueeze(0)\n",
    "\n",
    "        jacobian = (eye / norm.unsqueeze(2)) - (quats_outer / norm_cube.unsqueeze(2))\n",
    "        grad_input = torch.einsum('nij,nj->ni', jacobian, grad_output)\n",
    "        return grad_input\n",
    "\n",
    "def test_normalize_quats(quats: torch.Tensor):\n",
    "    return quats / quats.norm(dim=1, keepdim=True)\n",
    "\n",
    "# Helper function to compare gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# Test cases\n",
    "def run_tests():\n",
    "    tolerance = 1e-6\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    quats = torch.tensor([[1.0, 0.0, 0.0, 0.0]], requires_grad=True, dtype=torch.float64)\n",
    "    gradcheck_passed = gradcheck(normalize_quats.apply, (quats,))\n",
    "    assert gradcheck_passed, \"Gradcheck failed!\"\n",
    "    print(\"Gradcheck passed.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd (single quaternion)\n",
    "    quats = torch.tensor([[1.0, 0.0, 0.0, 0.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = normalize_quats.apply(quats)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = quats.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    quats.grad.zero_()\n",
    "    output = test_normalize_quats(quats)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = quats.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"My grads do not match autograd grads (single quaternion)!\"\n",
    "    print(\"My grads match autograd grads (single quaternion).\")\n",
    "\n",
    "    # 3. Test with multiple quaternions\n",
    "    quats = torch.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.5, 0.5, 0.5, 0.5]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = normalize_quats.apply(quats)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = quats.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    quats.grad.zero_()\n",
    "    output = test_normalize_quats(quats)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = quats.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"My grads do not match autograd grads (multiple quaternions)!\"\n",
    "    print(\"My grads match autograd grads (multiple quaternions).\")\n",
    "\n",
    "    # 4. Edge case: Small and large values\n",
    "    quats = torch.tensor([[1e-5, 1e-5, 1e-5, 1e-5], [1e5, 0.0, 0.0, 0.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = normalize_quats.apply(quats)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = quats.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    quats.grad.zero_()\n",
    "    output = test_normalize_quats(quats)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = quats.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"My grads do not match autograd grads (edge cases)!\"\n",
    "    print(\"My grads match autograd grads (edge cases).\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed.\n",
      "My grads match autograd grads (single input).\n",
      "My grads match autograd grads (multiple inputs).\n",
      "My grads match autograd grads (edge cases).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "class scale_to_s_matrix(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, s: torch.Tensor):\n",
    "        \"\"\"Takes the nx3 tensor and returns the nx3x3 diagonal matrix\"\"\"\n",
    "        ctx.save_for_backward(s)\n",
    "        return torch.diag_embed(s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        \"\"\"Grad output is a nx3x3 tensor\"\"\"\n",
    "        s = ctx.saved_tensors[0]\n",
    "        deriv_wr_s1 = grad_output[:, 0, 0].view(-1, 1)\n",
    "        deriv_wr_s2 = grad_output[:, 1, 1].view(-1, 1)\n",
    "        deriv_wr_s3 = grad_output[:, 2, 2].view(-1, 1)\n",
    "        return torch.cat([deriv_wr_s1, deriv_wr_s2, deriv_wr_s3], dim=1)\n",
    "\n",
    "def test_scale_to_s_matrix(s: torch.Tensor):\n",
    "    return torch.diag_embed(s)\n",
    "\n",
    "# Helper function to compare gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# Test cases\n",
    "def run_tests():\n",
    "    tolerance = 1e-6\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    s = torch.tensor([[1.0, 2.0, 3.0]], requires_grad=True, dtype=torch.float64)\n",
    "    gradcheck_passed = gradcheck(scale_to_s_matrix.apply, (s,))\n",
    "    assert gradcheck_passed, \"Gradcheck failed!\"\n",
    "    print(\"Gradcheck passed.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd (single input)\n",
    "    s = torch.tensor([[1.0, 2.0, 3.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = scale_to_s_matrix.apply(s)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = s.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    s.grad.zero_()\n",
    "    output = test_scale_to_s_matrix(s)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = s.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"My grads do not match autograd grads (single input)!\"\n",
    "    print(\"My grads match autograd grads (single input).\")\n",
    "\n",
    "    # 3. Test with multiple inputs\n",
    "    s = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = scale_to_s_matrix.apply(s)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = s.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    s.grad.zero_()\n",
    "    output = test_scale_to_s_matrix(s)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = s.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"My grads do not match autograd grads (multiple inputs)!\"\n",
    "    print(\"My grads match autograd grads (multiple inputs).\")\n",
    "\n",
    "    # 4. Edge case: Small and large values\n",
    "    s = torch.tensor([[1e-5, 1e-5, 1e-5], [1e5, 1e5, 1e5]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = scale_to_s_matrix.apply(s)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = s.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    s.grad.zero_()\n",
    "    output = test_scale_to_s_matrix(s)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = s.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"My grads do not match autograd grads (edge cases)!\"\n",
    "    print(\"My grads match autograd grads (edge cases).\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed.\n",
      "My grads match autograd grads (single value).\n",
      "My grads match autograd grads (multiple values).\n",
      "My grads match autograd grads (edge cases).\n",
      "My grads match autograd grads (negative values).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "class scaling_exp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, s: torch.Tensor):\n",
    "        ctx.save_for_backward(s)\n",
    "        return torch.exp(s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        s = ctx.saved_tensors[0]\n",
    "        return grad_output * torch.exp(s)\n",
    "\n",
    "# Define a test function using PyTorch's autograd\n",
    "def test_scaling_exp(s: torch.Tensor):\n",
    "    return torch.exp(s)\n",
    "\n",
    "# Helper function to compare gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# Test cases\n",
    "def run_tests():\n",
    "    tolerance = 1e-6\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    s = torch.tensor([1.0], requires_grad=True, dtype=torch.float64)\n",
    "    gradcheck_passed = gradcheck(scaling_exp.apply, (s,))\n",
    "    assert gradcheck_passed, \"Gradcheck failed!\"\n",
    "    print(\"Gradcheck passed.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd (single value)\n",
    "    s = torch.tensor([1.0], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = scaling_exp.apply(s)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = s.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    s.grad.zero_()\n",
    "    output = test_scaling_exp(s)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = s.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"My grads do not match autograd grads (single value)!\"\n",
    "    print(\"My grads match autograd grads (single value).\")\n",
    "\n",
    "    # 3. Test with multiple values\n",
    "    s = torch.tensor([1.0, 2.0, 3.0], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = scaling_exp.apply(s)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = s.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    s.grad.zero_()\n",
    "    output = test_scaling_exp(s)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = s.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"My grads do not match autograd grads (multiple values)!\"\n",
    "    print(\"My grads match autograd grads (multiple values).\")\n",
    "\n",
    "    # 4. Edge case: Very small and very large values\n",
    "    s = torch.tensor([1e-5, 1e5], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = scaling_exp.apply(s)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = s.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    s.grad.zero_()\n",
    "    output = test_scaling_exp(s)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = s.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"My grads do not match autograd grads (edge cases)!\"\n",
    "    print(\"My grads match autograd grads (edge cases).\")\n",
    "\n",
    "    # 5. Test with a tensor containing negative values\n",
    "    s = torch.tensor([-1.0, -2.0, -3.0], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = scaling_exp.apply(s)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads = s.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    s.grad.zero_()\n",
    "    output = test_scaling_exp(s)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads = s.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads, autograd_grads, tolerance), \"My grads do not match autograd grads (negative values)!\"\n",
    "    print(\"My grads match autograd grads (negative values).\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed.\n",
      "My grads match autograd grads.\n",
      "Edge case (batch size 1) passed.\n",
      "Edge case (very small/large values) passed.\n",
      "Zero matrices test passed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "class mean_3d_to_camera_space(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, mean_3d: torch.Tensor, extrinsic_matrix: torch.Tensor):\n",
    "        ctx.save_for_backward(extrinsic_matrix)\n",
    "        return torch.einsum(\"nk, kh->nh\", mean_3d, extrinsic_matrix)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        extrinsic_matrix = ctx.saved_tensors[0]\n",
    "        mean_3d_grad = torch.einsum(\"nh,hj->nj\", grad_output, extrinsic_matrix.transpose(0, 1))\n",
    "        return mean_3d_grad, None\n",
    "\n",
    "# Helper function to compare gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# Define a test function using PyTorch's autograd\n",
    "def test_mean_3d_to_camera_space(mean_3d, extrinsic_matrix):\n",
    "    return torch.einsum(\"nk, kh->nh\", mean_3d, extrinsic_matrix)\n",
    "\n",
    "# Test cases\n",
    "def run_tests():\n",
    "    tolerance = 1e-6\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    mean_3d = torch.rand(1, 4, requires_grad=True, dtype=torch.float64)\n",
    "    extrinsic_matrix = torch.rand(4, 4, requires_grad=False, dtype=torch.float64)\n",
    "    gradcheck_passed = gradcheck(mean_3d_to_camera_space.apply, (mean_3d, extrinsic_matrix))\n",
    "    assert gradcheck_passed, \"Gradcheck failed!\"\n",
    "    print(\"Gradcheck passed.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd\n",
    "    mean_3d = torch.rand(1, 4, requires_grad=True, dtype=torch.float64)\n",
    "    extrinsic_matrix = torch.rand(4, 4, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = mean_3d_to_camera_space.apply(mean_3d, extrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_mean_3d = mean_3d.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    mean_3d.grad.zero_()\n",
    "    output = test_mean_3d_to_camera_space(mean_3d, extrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_mean_3d = mean_3d.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_mean_3d, autograd_grads_mean_3d, tolerance), \"My grads do not match autograd grads (mean_3d)!\"\n",
    "    print(\"My grads match autograd grads.\")\n",
    "\n",
    "    # 3. Edge case: Batch size of 1\n",
    "    mean_3d = torch.rand(1, 4, requires_grad=True, dtype=torch.float64)\n",
    "    extrinsic_matrix = torch.rand(4, 4, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    output = mean_3d_to_camera_space.apply(mean_3d, extrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    print(\"Edge case (batch size 1) passed.\")\n",
    "\n",
    "    # 4. Edge case: Very small and very large values\n",
    "    mean_3d = torch.tensor([[1e-5, 1e5, -1e5, 1.0]], requires_grad=True, dtype=torch.float64)\n",
    "    extrinsic_matrix = torch.rand(4, 4, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    output = mean_3d_to_camera_space.apply(mean_3d, extrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    print(\"Edge case (very small/large values) passed.\")\n",
    "\n",
    "    # 5. Test with zero matrices\n",
    "    mean_3d = torch.zeros(3, 4, requires_grad=True, dtype=torch.float64)\n",
    "    extrinsic_matrix = torch.zeros(4, 4, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    output = mean_3d_to_camera_space.apply(mean_3d, extrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    assert torch.all(mean_3d.grad == 0), \"Gradients for mean_3d should be zero!\"\n",
    "    print(\"Zero matrices test passed.\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed.\n",
      "My grads match autograd grads.\n",
      "Edge case (batch size 1) passed.\n",
      "Edge case (very small/large values) passed.\n",
      "Zero matrices test passed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "\n",
    "class camera_space_to_pixel_space(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, mean_3d: torch.Tensor, intrinsic_matrix: torch.Tensor):\n",
    "        ctx.save_for_backward(intrinsic_matrix)\n",
    "        return torch.einsum(\"nk, kh->nh\", mean_3d, intrinsic_matrix)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):   \n",
    "        intrinsic_matrix = ctx.saved_tensors[0]\n",
    "        mean_3d_grad = torch.einsum(\"nh,hj->nj\", grad_output, intrinsic_matrix.transpose(0, 1))\n",
    "        return mean_3d_grad, None\n",
    "\n",
    "# Helper function to compare gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# Define a test function using PyTorch's autograd\n",
    "def test_camera_space_to_pixel_space(mean_3d, intrinsic_matrix):\n",
    "    return torch.einsum(\"nk, kh->nh\", mean_3d, intrinsic_matrix)\n",
    "\n",
    "# Test cases\n",
    "def run_tests():\n",
    "    tolerance = 1e-6\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    mean_3d = torch.rand(3, 4, requires_grad=True, dtype=torch.float64)\n",
    "    intrinsic_matrix = torch.rand(4, 4, requires_grad=False, dtype=torch.float64)\n",
    "    gradcheck_passed = gradcheck(camera_space_to_pixel_space.apply, (mean_3d, intrinsic_matrix))\n",
    "    assert gradcheck_passed, \"Gradcheck failed!\"\n",
    "    print(\"Gradcheck passed.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd\n",
    "    mean_3d = torch.rand(3, 4, requires_grad=True, dtype=torch.float64)\n",
    "    intrinsic_matrix = torch.rand(4, 4, requires_grad=False, dtype=torch.float64)\n",
    "\n",
    "    # My grads\n",
    "    output = camera_space_to_pixel_space.apply(mean_3d, intrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_mean_3d = mean_3d.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    mean_3d.grad.zero_()\n",
    "    output = test_camera_space_to_pixel_space(mean_3d, intrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_mean_3d = mean_3d.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_mean_3d, autograd_grads_mean_3d, tolerance), \"My grads do not match autograd grads (mean_3d)!\"\n",
    "    print(\"My grads match autograd grads.\")\n",
    "\n",
    "    # 3. Edge case: Batch size of 1\n",
    "    mean_3d = torch.rand(1, 4, requires_grad=True, dtype=torch.float64)\n",
    "    intrinsic_matrix = torch.rand(4, 4, requires_grad=False, dtype=torch.float64)\n",
    "\n",
    "    output = camera_space_to_pixel_space.apply(mean_3d, intrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    print(\"Edge case (batch size 1) passed.\")\n",
    "\n",
    "    # 4. Edge case: Very small and very large values\n",
    "    mean_3d = torch.tensor([[1e-5, 1e5, -1e5, 1.0]], requires_grad=True, dtype=torch.float64)\n",
    "    intrinsic_matrix = torch.rand(4, 4, requires_grad=False, dtype=torch.float64)\n",
    "\n",
    "    output = camera_space_to_pixel_space.apply(mean_3d, intrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    print(\"Edge case (very small/large values) passed.\")\n",
    "\n",
    "    # 5. Test with zero matrices\n",
    "    mean_3d = torch.zeros(3, 4, requires_grad=True, dtype=torch.float64)\n",
    "    intrinsic_matrix = torch.zeros(4, 4, requires_grad=False, dtype=torch.float64)\n",
    "\n",
    "    output = camera_space_to_pixel_space.apply(mean_3d, intrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    assert torch.all(mean_3d.grad == 0), \"Gradients for mean_3d should be zero!\"\n",
    "    print(\"Zero matrices test passed.\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed.\n",
      "My grads match autograd grads.\n",
      "Edge case (boundaries) passed.\n",
      "tensor([[639.5000, 359.5000,   0.0000]], dtype=torch.float64,\n",
      "       grad_fn=<ndc_to_pixelsBackward>)\n",
      "Edge case (single point) passed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "class ndc_to_pixels(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, ndc: torch.Tensor, dimension: list):\n",
    "        \"\"\"ndc is a nx3 tensor where the last dimension is the z component\n",
    "        \n",
    "        dimension are the height and width of the image\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(torch.tensor(dimension))\n",
    "        ndc = ndc.clone()  # To avoid modifying input in-place\n",
    "        ndc[:, 0] = (ndc[:, 0] + 1) * (dimension[1] - 1) * 0.5\n",
    "        ndc[:, 1] = (ndc[:, 1] + 1) * (dimension[0] - 1) * 0.5\n",
    "        return ndc\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        dimension = ctx.saved_tensors[0]\n",
    "        grad_ndc = grad_output.clone()\n",
    "\n",
    "        # Compute the gradient for ndc\n",
    "        grad_ndc[:, 0] *= (dimension[1] - 1) * 0.5\n",
    "        grad_ndc[:, 1] *= (dimension[0] - 1) * 0.5\n",
    "        # grad_ndc[:, 2] = 0  # z-component has no effect on pixel coordinates\n",
    "        return grad_ndc, None\n",
    "\n",
    "# Helper function to compare gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# Define a test function using PyTorch's autograd\n",
    "def test_ndc_to_pixels(ndc: torch.Tensor, dimension: list):\n",
    "    ndc = ndc.clone()\n",
    "    ndc[:, 0] = (ndc[:, 0] + 1) * (dimension[1] - 1) * 0.5\n",
    "    ndc[:, 1] = (ndc[:, 1] + 1) * (dimension[0] - 1) * 0.5\n",
    "    return ndc\n",
    "\n",
    "# Test cases\n",
    "def run_tests():\n",
    "    tolerance = 1e-6\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    ndc = torch.rand(5, 3, requires_grad=True, dtype=torch.float64)\n",
    "    dimension = [720, 1280]\n",
    "    gradcheck_passed = gradcheck(ndc_to_pixels.apply, (ndc, dimension))\n",
    "    assert gradcheck_passed, \"Gradcheck failed!\"\n",
    "    print(\"Gradcheck passed.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd\n",
    "    ndc = torch.rand(5, 3, requires_grad=True, dtype=torch.float64)\n",
    "    dimension = [720, 1280]\n",
    "\n",
    "    # My grads\n",
    "    output = ndc_to_pixels.apply(ndc, dimension)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_ndc = ndc.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    ndc.grad.zero_()\n",
    "    output = test_ndc_to_pixels(ndc, dimension)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_ndc = ndc.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_ndc, autograd_grads_ndc, tolerance), \"My grads do not match autograd grads!\"\n",
    "    print(\"My grads match autograd grads.\")\n",
    "\n",
    "    # 3. Edge case: NDC values at boundaries\n",
    "    ndc = torch.tensor([[-1.0, -1.0, 0.0], [1.0, 1.0, 0.0]], requires_grad=True, dtype=torch.float64)\n",
    "    dimension = [720, 1280]\n",
    "\n",
    "    output = ndc_to_pixels.apply(ndc, dimension)\n",
    "    assert torch.all(output[:, 0] == torch.tensor([0.0, 1279.0], dtype=torch.float64)), \"X coordinates incorrect at boundaries!\"\n",
    "    assert torch.all(output[:, 1] == torch.tensor([0.0, 719.0], dtype=torch.float64)), \"Y coordinates incorrect at boundaries!\"\n",
    "    print(\"Edge case (boundaries) passed.\")\n",
    "\n",
    "    # 4. Edge case: Single NDC point\n",
    "    ndc = torch.tensor([[0.0, 0.0, 0.0]], requires_grad=True, dtype=torch.float64)\n",
    "    dimension = [720, 1280]\n",
    "\n",
    "    output = ndc_to_pixels.apply(ndc, dimension)\n",
    "    print(output)\n",
    "    assert torch.allclose(output[0, :2], torch.tensor([639.0, 359.0], dtype=torch.float64), atol=1e-2, rtol=1e-2), \"Incorrect result for single NDC point!\"\n",
    "    print(\"Edge case (single point) passed.\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
