{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.]], device='cuda:0')\n",
      "Gradcheck passed for final_color.\n",
      "Gradcheck passed for final_color_pytorch_gradcheck.\n",
      "Gradients match for autograd.\n",
      "Edge case test passed for zero tensors.\n",
      "Gradients match for large batch size.\n",
      "Extreme value test passed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "from splat.custom_backwards_implementation.gaussian_weight_derivatives import (\n",
    "    backward_final_color_launcher,\n",
    ")\n",
    "from splat.utils import build_rotation\n",
    "\n",
    "\n",
    "class final_color(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, color: torch.Tensor, current_T: torch.Tensor, alpha: torch.Tensor):\n",
    "        \"\"\"Color is a nx3 tensor, weight is a nx1 tensor, alpha is a nx1 tensor\"\"\"\n",
    "        ctx.save_for_backward(color, current_T, alpha)\n",
    "        return color * current_T * alpha\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        \"\"\"Output of forward is a nx3 tensor so the grad_output is a nx3 tensor\"\"\"\n",
    "        color, current_T, alpha = ctx.saved_tensors\n",
    "        print(grad_output)\n",
    "        grad_color = torch.zeros_like(color).to(color.device)\n",
    "        grad_alpha = torch.zeros_like(alpha).to(alpha.device)\n",
    "        backward_final_color_launcher(\n",
    "            grad_output.contiguous(),\n",
    "            color.contiguous(),\n",
    "            current_T.contiguous(),\n",
    "            alpha.contiguous(),\n",
    "            grad_color.contiguous(),\n",
    "            grad_alpha.contiguous()\n",
    "        )\n",
    "        return grad_color, None, grad_alpha\n",
    "\n",
    "class final_color_pytorch_gradcheck(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, color: torch.Tensor, current_T: torch.Tensor, alpha: torch.Tensor):\n",
    "        \"\"\"Color is a nx3 tensor, weight is a nx1 tensor, alpha is a nx1 tensor\"\"\"\n",
    "        ctx.save_for_backward(color, current_T, alpha)\n",
    "        return color * current_T * alpha\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        \"\"\"Output of forward is a nx3 tensor so the grad_output is a nx3 tensor\"\"\"\n",
    "        color, current_T, alpha = ctx.saved_tensors\n",
    "        grad_color = grad_output * current_T * alpha\n",
    "        grad_alpha = (grad_output * color * current_T).sum(dim=1, keepdim=True)\n",
    "        return grad_color, None, grad_alpha\n",
    "    \n",
    "def autograd_test(color: torch.Tensor, current_T: torch.Tensor, alpha: torch.Tensor):\n",
    "    return color * current_T * alpha\n",
    "\n",
    "# Define helper function for comparing gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# first we check with gradcheck\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32\n",
    "def run_tests():\n",
    "    tolerance = 1e-4\n",
    "    eps = 1e-3\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dtype = torch.float32\n",
    "\n",
    "    color = torch.tensor([[1.0, 2.0, 3.0]], requires_grad=True, dtype=dtype, device=device)\n",
    "    current_T = torch.tensor([[4.0]], dtype=dtype, device=device)\n",
    "    alpha = torch.tensor([[5.0]], requires_grad=True, dtype=dtype, device=device)\n",
    "    \n",
    "    # Gradcheck for both implementations\n",
    "    assert gradcheck(final_color.apply, (color, current_T, alpha), eps=eps), \"Gradcheck failed for final_color!\"\n",
    "    print(\"Gradcheck passed for final_color.\")\n",
    "\n",
    "    assert gradcheck(final_color_pytorch_gradcheck.apply, (color, current_T, alpha), eps=eps), \"Gradcheck failed for final_color_pytorch_gradcheck!\"\n",
    "    print(\"Gradcheck passed for final_color_pytorch_gradcheck.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd\n",
    "    color = torch.randn((1, 3), requires_grad=True, dtype=dtype, device=device)\n",
    "    current_T = torch.ones((1, 1), requires_grad=False, dtype=dtype, device=device)\n",
    "    alpha = torch.rand((1, 1), requires_grad=True, dtype=dtype, device=device)\n",
    "\n",
    "    # My grads\n",
    "    output = final_color.apply(color, current_T, alpha)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_color = color.grad.clone()\n",
    "    my_grads_alpha = alpha.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    color.grad.zero_()\n",
    "    alpha.grad.zero_()\n",
    "    output = autograd_test(color, current_T, alpha)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_color = color.grad.clone()\n",
    "    autograd_grads_alpha = alpha.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_color, autograd_grads_color, tolerance), \"Mismatch in gradients for color!\"\n",
    "    assert compare_grads(my_grads_alpha, autograd_grads_alpha, tolerance), \"Mismatch in gradients for alpha!\"\n",
    "    print(\"Gradients match for autograd.\")\n",
    "\n",
    "    # 3. Test edge cases: zero tensors\n",
    "    color = torch.zeros((1, 3), requires_grad=True, dtype=dtype, device=device)\n",
    "    current_T = torch.zeros((1, 1), requires_grad=False, dtype=dtype, device=device)\n",
    "    alpha = torch.zeros((1, 1), requires_grad=True, dtype=dtype, device=device)\n",
    "\n",
    "    output = final_color.apply(color, current_T, alpha)\n",
    "    assert torch.all(output == 0), \"Output is not zero for zero input tensors.\"\n",
    "    print(\"Edge case test passed for zero tensors.\")\n",
    "\n",
    "    # 4. Test with large batch sizes\n",
    "    batch_size = 10\n",
    "    color = torch.randn((batch_size, 3), requires_grad=True, dtype=dtype, device=device)\n",
    "    current_T = torch.ones((batch_size, 1), requires_grad=False, dtype=dtype, device=device)\n",
    "    alpha = torch.rand((batch_size, 1), requires_grad=True, dtype=dtype, device=device)\n",
    "\n",
    "    # My grads\n",
    "    output = final_color.apply(color, current_T, alpha)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_color = color.grad.clone()\n",
    "    my_grads_alpha = alpha.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    color.grad.zero_()\n",
    "    alpha.grad.zero_()\n",
    "    output = autograd_test(color, current_T, alpha)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_color = color.grad.clone()\n",
    "    autograd_grads_alpha = alpha.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_color, autograd_grads_color, tolerance), \"Mismatch in gradients for large batch size (color)!\"\n",
    "    assert compare_grads(my_grads_alpha, autograd_grads_alpha, tolerance), \"Mismatch in gradients for large batch size (alpha)!\"\n",
    "    print(\"Gradients match for large batch size.\")\n",
    "\n",
    "    # 5. Test with extreme values\n",
    "    color = torch.full((10, 3), 1e6, requires_grad=True, dtype=dtype, device=device)\n",
    "    current_T = torch.full((10, 1), 1e-6, requires_grad=False, dtype=dtype, device=device)\n",
    "    alpha = torch.full((10, 1), 1e6, requires_grad=True, dtype=dtype, device=device)\n",
    "\n",
    "    output = final_color.apply(color, current_T, alpha)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    assert torch.isfinite(color.grad).all(), \"Gradients for color are not finite with extreme values!\"\n",
    "    assert torch.isfinite(alpha.grad).all(), \"Gradients for alpha are not finite with extreme values!\"\n",
    "    print(\"Extreme value test passed.\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da2986/intro_to_gaussian_splatting/.venv/lib/python3.10/site-packages/torch/autograd/gradcheck.py:652: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. \n",
      "  warnings.warn(\n",
      "/home/da2986/intro_to_gaussian_splatting/.venv/lib/python3.10/site-packages/torch/autograd/gradcheck.py:652: UserWarning: Input #1 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed for get_alpha.\n",
      "Gradients match for autograd.\n",
      "Edge case test passed for zero tensors.\n",
      "Gradients match for large batch size.\n",
      "Extreme value test passed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "from splat.custom_backwards_implementation.gaussian_weight_derivatives import (\n",
    "    get_alpha_backward_launcher,\n",
    ")\n",
    "\n",
    "class get_alpha(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, gaussian_strength: torch.Tensor, unactivated_opacity: torch.Tensor):\n",
    "        \"\"\"Gaussian strength is a nx1 tensor, unactivated opacity is a nx1 tensor\"\"\"\n",
    "        ctx.save_for_backward(gaussian_strength, unactivated_opacity)\n",
    "        activated_opacity = torch.sigmoid(unactivated_opacity)\n",
    "        return gaussian_strength * activated_opacity\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        \"\"\"Output of forward is a nx1 tensor so the grad_output is a nx1 tensor\"\"\"\n",
    "        gaussian_strength, unactivated_opacity = ctx.saved_tensors\n",
    "        grad_gaussian_strength = torch.zeros_like(gaussian_strength).to(gaussian_strength.device)\n",
    "        grad_unactivated_opacity = torch.zeros_like(unactivated_opacity).to(unactivated_opacity.device)\n",
    "        get_alpha_backward_launcher(\n",
    "            grad_output.contiguous(),\n",
    "            gaussian_strength.contiguous(),\n",
    "            unactivated_opacity.contiguous(),\n",
    "            grad_gaussian_strength.contiguous(),\n",
    "            grad_unactivated_opacity.contiguous()\n",
    "        )\n",
    "        return grad_gaussian_strength, grad_unactivated_opacity\n",
    "# Define a test function using torch's autograd\n",
    "def autograd_test(gaussian_strength: torch.Tensor, unactivated_opacity: torch.Tensor):\n",
    "    activated_opacity = torch.sigmoid(unactivated_opacity)\n",
    "    return gaussian_strength * activated_opacity\n",
    "\n",
    "# Define helper function for comparing gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "def run_tests():\n",
    "    tolerance = 1e-4\n",
    "    eps = 1e-3\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dtype = torch.float32\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    gaussian_strength = torch.tensor([[2.0]], requires_grad=True, dtype=torch.float32, device=device)\n",
    "    unactivated_opacity = torch.tensor([[3.0]], requires_grad=True, dtype=torch.float32, device=device)\n",
    "\n",
    "    assert gradcheck(get_alpha.apply, (gaussian_strength, unactivated_opacity), eps=eps), \"Gradcheck failed for get_alpha!\"\n",
    "    print(\"Gradcheck passed for get_alpha.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd\n",
    "    gaussian_strength = torch.randn((1, 1), requires_grad=True, dtype=dtype, device=device)\n",
    "    unactivated_opacity = torch.randn((1, 1), requires_grad=True, dtype=dtype, device=device)\n",
    "\n",
    "    # My grads\n",
    "    output = get_alpha.apply(gaussian_strength, unactivated_opacity)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_gaussian_strength = gaussian_strength.grad.clone()\n",
    "    my_grads_unactivated_opacity = unactivated_opacity.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    gaussian_strength.grad.zero_()\n",
    "    unactivated_opacity.grad.zero_()\n",
    "    output = autograd_test(gaussian_strength, unactivated_opacity)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_gaussian_strength = gaussian_strength.grad.clone()\n",
    "    autograd_grads_unactivated_opacity = unactivated_opacity.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_gaussian_strength, autograd_grads_gaussian_strength, tolerance), \"Mismatch in gradients for gaussian_strength!\"\n",
    "    assert compare_grads(my_grads_unactivated_opacity, autograd_grads_unactivated_opacity, tolerance), \"Mismatch in gradients for unactivated_opacity!\"\n",
    "    print(\"Gradients match for autograd.\")\n",
    "\n",
    "    # 3. Test edge cases: zero tensors\n",
    "    gaussian_strength = torch.zeros((1, 1), requires_grad=True, dtype=dtype, device=device)\n",
    "    unactivated_opacity = torch.zeros((1, 1), requires_grad=True, dtype=dtype, device=device)\n",
    "\n",
    "    output = get_alpha.apply(gaussian_strength, unactivated_opacity)\n",
    "    assert torch.all(output == 0), \"Output is not zero for zero input tensors.\"\n",
    "    print(\"Edge case test passed for zero tensors.\")\n",
    "\n",
    "    # 4. Test with large batch sizes\n",
    "    batch_size = 10\n",
    "    gaussian_strength = torch.randn((batch_size, 1), requires_grad=True, dtype=dtype, device=device)\n",
    "    unactivated_opacity = torch.randn((batch_size, 1), requires_grad=True, dtype=dtype, device=device)\n",
    "\n",
    "    # My grads\n",
    "    output = get_alpha.apply(gaussian_strength, unactivated_opacity)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_gaussian_strength = gaussian_strength.grad.clone()\n",
    "    my_grads_unactivated_opacity = unactivated_opacity.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    gaussian_strength.grad.zero_()\n",
    "    unactivated_opacity.grad.zero_()\n",
    "    output = autograd_test(gaussian_strength, unactivated_opacity)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_gaussian_strength = gaussian_strength.grad.clone()\n",
    "    autograd_grads_unactivated_opacity = unactivated_opacity.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_gaussian_strength, autograd_grads_gaussian_strength, tolerance), \"Mismatch in gradients for gaussian_strength (large batch size)!\"\n",
    "    assert compare_grads(my_grads_unactivated_opacity, autograd_grads_unactivated_opacity, tolerance), \"Mismatch in gradients for unactivated_opacity (large batch size)!\"\n",
    "    print(\"Gradients match for large batch size.\")\n",
    "\n",
    "    # 5. Test with extreme values\n",
    "    gaussian_strength = torch.full((10, 1), 1e6, requires_grad=True, dtype=dtype, device=device)\n",
    "    unactivated_opacity = torch.full((10, 1), -1e6, requires_grad=True, dtype=dtype, device=device)  # Test sigmoid near zero\n",
    "\n",
    "    output = get_alpha.apply(gaussian_strength, unactivated_opacity)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    assert torch.isfinite(gaussian_strength.grad).all(), \"Gradients for gaussian_strength are not finite with extreme values!\"\n",
    "    assert torch.isfinite(unactivated_opacity.grad).all(), \"Gradients for unactivated_opacity are not finite with extreme values!\"\n",
    "    print(\"Extreme value test passed.\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed for gaussian_exp.\n",
      "Gradients match for autograd.\n",
      "Edge case test passed for zero tensors.\n",
      "Gradients match for large batch size.\n",
      "Extreme value test passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da2986/intro_to_gaussian_splatting/.venv/lib/python3.10/site-packages/torch/autograd/gradcheck.py:652: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "from splat.custom_backwards_implementation.gaussian_weight_derivatives import (\n",
    "    gaussian_exp_backward_launcher,\n",
    ")\n",
    "\n",
    "class gaussian_exp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, gaussian_weight: torch.Tensor):\n",
    "        ctx.save_for_backward(gaussian_weight)\n",
    "        return torch.exp(gaussian_weight)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        gaussian_weight, = ctx.saved_tensors\n",
    "        grad_gaussian_weight = torch.zeros_like(gaussian_weight).to(gaussian_weight.device)\n",
    "        gaussian_exp_backward_launcher(\n",
    "            grad_output.contiguous(),\n",
    "            gaussian_weight.contiguous(),\n",
    "            grad_gaussian_weight.contiguous()\n",
    "        )\n",
    "        return grad_gaussian_weight\n",
    "\n",
    "# Define a test function using torch's autograd\n",
    "def autograd_test(gaussian_weight: torch.Tensor):\n",
    "    return torch.exp(gaussian_weight)\n",
    "\n",
    "# Helper function to compare gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "def run_tests():\n",
    "    tolerance = 1e-4\n",
    "    eps = 1e-3\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dtype = torch.float32\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    gaussian_weight = torch.tensor([[2.0]], requires_grad=True, dtype=torch.float32, device=device)\n",
    "\n",
    "    assert gradcheck(gaussian_exp.apply, (gaussian_weight,), eps=eps), \"Gradcheck failed for gaussian_exp!\"\n",
    "    print(\"Gradcheck passed for gaussian_exp.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd\n",
    "    gaussian_weight = torch.randn((1, 1), requires_grad=True, dtype=dtype, device=device)\n",
    "\n",
    "    # My grads\n",
    "    output = gaussian_exp.apply(gaussian_weight)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_gaussian_weight = gaussian_weight.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    gaussian_weight.grad.zero_()\n",
    "    output = autograd_test(gaussian_weight)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_gaussian_weight = gaussian_weight.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_gaussian_weight, autograd_grads_gaussian_weight, tolerance), \"Mismatch in gradients for gaussian_weight!\"\n",
    "    print(\"Gradients match for autograd.\")\n",
    "\n",
    "    # 3. Test edge cases: zero tensors\n",
    "    gaussian_weight = torch.zeros((1, 1), requires_grad=True, dtype=dtype, device=device)\n",
    "\n",
    "    output = gaussian_exp.apply(gaussian_weight)\n",
    "    expected_output = torch.ones_like(gaussian_weight)\n",
    "    assert torch.allclose(output, expected_output, atol=tolerance, rtol=tolerance), \"Output mismatch for zero input tensors.\"\n",
    "    print(\"Edge case test passed for zero tensors.\")\n",
    "\n",
    "    # 4. Test with large batch sizes\n",
    "    batch_size = 10\n",
    "    gaussian_weight = torch.randn((batch_size, 1), requires_grad=True, dtype=dtype, device=device)\n",
    "\n",
    "    # My grads\n",
    "    output = gaussian_exp.apply(gaussian_weight)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_gaussian_weight = gaussian_weight.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    gaussian_weight.grad.zero_()\n",
    "    output = autograd_test(gaussian_weight)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_gaussian_weight = gaussian_weight.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_gaussian_weight, autograd_grads_gaussian_weight, tolerance), \"Mismatch in gradients for large batch size!\"\n",
    "    print(\"Gradients match for large batch size.\")\n",
    "\n",
    "    # 5. Test with extreme values\n",
    "    gaussian_weight = torch.full((10, 1), 1e1, requires_grad=True, dtype=dtype, device=device)\n",
    "\n",
    "    output = gaussian_exp.apply(gaussian_weight)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    assert torch.isfinite(gaussian_weight.grad).all(), \"Gradients for gaussian_weight are not finite with extreme values!\"\n",
    "    print(\"Extreme value test passed.\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed for gaussian_weight.\n",
      "Gradients match for autograd.\n",
      "Edge case test passed for zero tensors.\n",
      "Gradients match for large batch size.\n",
      "Extreme value test passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da2986/intro_to_gaussian_splatting/.venv/lib/python3.10/site-packages/torch/autograd/gradcheck.py:652: UserWarning: Input #1 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "import torch.nn as nn\n",
    "\n",
    "from splat.custom_backwards_implementation.gaussian_weight_derivatives import (\n",
    "    gaussian_weight_grad_inv_cov_launcher,\n",
    "    gaussian_weight_grad_gaussian_mean_launcher,\n",
    ")\n",
    "\n",
    "class gaussian_weight(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, gaussian_mean: torch.Tensor, inverted_covariance: torch.Tensor, pixel: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Pixel means are a nx2 tensor, inverted covariance is a 2x2 tensor, pixel is a nx2 tensor\n",
    "        Outputs a nx1 tensor\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(gaussian_mean, inverted_covariance, pixel)\n",
    "        diff = (pixel - gaussian_mean).unsqueeze(1)\n",
    "\n",
    "        inv_cov_mult = torch.einsum('bij,bjk->bik', inverted_covariance, diff.transpose(1, 2))\n",
    "        return -0.5 * torch.einsum('bij,bjk->bik', diff, inv_cov_mult).squeeze(-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        \"\"\"Output of forward is a nx1 tensor so the grad_output is a nx1 tensor\"\"\"\n",
    "        gaussian_mean, inverted_covariance, pixel = ctx.saved_tensors\n",
    "        diff = (pixel - gaussian_mean).unsqueeze(1)  # nx1x2\n",
    "\n",
    "        grad_inv_cov = torch.zeros_like(inverted_covariance)\n",
    "        grad_gaussian_mean = torch.zeros_like(gaussian_mean)\n",
    "        gaussian_weight_grad_inv_cov_launcher(\n",
    "            grad_output.contiguous(),\n",
    "            diff.contiguous(),\n",
    "            grad_inv_cov.contiguous()\n",
    "        )\n",
    "\n",
    "        gaussian_weight_grad_gaussian_mean_launcher(\n",
    "            grad_output.contiguous(),\n",
    "            diff.contiguous(),\n",
    "            inverted_covariance.contiguous(),\n",
    "            grad_gaussian_mean.contiguous()\n",
    "        )\n",
    "        return grad_gaussian_mean, grad_inv_cov, None\n",
    "    \n",
    "\n",
    "# Define a test function using PyTorch's autograd\n",
    "def autograd_test(gaussian_mean, inverted_covariance, pixel):\n",
    "    diff = (pixel - gaussian_mean).unsqueeze(1)\n",
    "    # 2x2 * 2x1 = 2x1\n",
    "    inv_cov_mult = torch.einsum('bij,bjk->bik', inverted_covariance, diff.transpose(1, 2))\n",
    "    return -0.5 * torch.einsum('bij,bjk->bik', diff, inv_cov_mult).squeeze(-1)\n",
    "\n",
    "# Helper function to compare gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "\n",
    "def run_tests():\n",
    "    tolerance = 1e-4\n",
    "    eps = 1e-3\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dtype = torch.float32\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    gaussian_mean = torch.tensor([[1.0, 2.0]], requires_grad=True, dtype=torch.float32, device=device)\n",
    "    inverted_covariance = torch.eye(2, requires_grad=True, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    pixel = torch.tensor([[3.0, 4.0]], requires_grad=False, dtype=torch.float32, device=device)\n",
    "\n",
    "    assert gradcheck(gaussian_weight.apply, (gaussian_mean, inverted_covariance, pixel), eps=eps), \"Gradcheck failed for gaussian_weight!\"\n",
    "    print(\"Gradcheck passed for gaussian_weight.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd\n",
    "    gaussian_mean = torch.randn((1, 2), requires_grad=True, dtype=dtype, device=device)\n",
    "    inverted_covariance = nn.Parameter(torch.eye(2, dtype=dtype, device=device).unsqueeze(0))   \n",
    "\n",
    "    pixel = torch.randn((1, 2), requires_grad=False, dtype=dtype, device=device)\n",
    "\n",
    "    # My grads\n",
    "    output = gaussian_weight.apply(gaussian_mean, inverted_covariance, pixel)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_gaussian_mean = gaussian_mean.grad.clone()\n",
    "    my_grads_inv_cov = inverted_covariance.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    gaussian_mean.grad.zero_()\n",
    "    inverted_covariance.grad.zero_()\n",
    "    output = autograd_test(gaussian_mean, inverted_covariance, pixel)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_gaussian_mean = gaussian_mean.grad.clone()\n",
    "    autograd_grads_inv_cov = inverted_covariance.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_gaussian_mean, autograd_grads_gaussian_mean, tolerance), \"Mismatch in gradients for gaussian_mean!\"\n",
    "    assert compare_grads(my_grads_inv_cov, autograd_grads_inv_cov, tolerance), \"Mismatch in gradients for inverted_covariance!\"\n",
    "    print(\"Gradients match for autograd.\")\n",
    "\n",
    "    # 3. Test edge cases: zero tensors\n",
    "    gaussian_mean = torch.zeros((1, 2), requires_grad=True, dtype=dtype, device=device)\n",
    "    inverted_covariance = torch.eye(2, requires_grad=True, dtype=dtype, device=device).unsqueeze(0).clone()\n",
    "    pixel = torch.zeros((1, 2), requires_grad=False, dtype=dtype, device=device)\n",
    "\n",
    "    output = gaussian_weight.apply(gaussian_mean, inverted_covariance, pixel)\n",
    "    assert torch.all(output == 0), \"Output is not zero for zero input tensors.\"\n",
    "    print(\"Edge case test passed for zero tensors.\")\n",
    "\n",
    "    # 4. Test with large batch sizes\n",
    "    batch_size = 10\n",
    "    gaussian_mean = torch.randn((batch_size, 2), requires_grad=True, dtype=dtype, device=device)\n",
    "    inverted_covariance = nn.Parameter(torch.stack([torch.eye(2, dtype=dtype, device=device) for _ in range(batch_size)], dim=0))\n",
    "    pixel = torch.randn((batch_size, 2), requires_grad=False, dtype=dtype, device=device)\n",
    "\n",
    "    # My grads\n",
    "    output = gaussian_weight.apply(gaussian_mean, inverted_covariance, pixel)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_gaussian_mean = gaussian_mean.grad.clone()\n",
    "    my_grads_inv_cov = inverted_covariance.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    gaussian_mean.grad.zero_()\n",
    "    inverted_covariance.grad.zero_()\n",
    "    output = autograd_test(gaussian_mean, inverted_covariance, pixel)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_gaussian_mean = gaussian_mean.grad.clone()\n",
    "    autograd_grads_inv_cov = inverted_covariance.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_gaussian_mean, autograd_grads_gaussian_mean, tolerance), \"Mismatch in gradients for gaussian_mean (large batch size)!\"\n",
    "    assert compare_grads(my_grads_inv_cov, autograd_grads_inv_cov, tolerance), \"Mismatch in gradients for inverted_covariance (large batch size)!\"\n",
    "    print(\"Gradients match for large batch size.\")\n",
    "\n",
    "    # 5. Test with extreme values\n",
    "    gaussian_mean = torch.full((10, 2), 1e6, requires_grad=True, dtype=dtype, device=device)\n",
    "    inverted_covariance = torch.stack([torch.eye(2, dtype=dtype, device=device) for _ in range(10)], dim=0).requires_grad_()\n",
    "    pixel = torch.full((10, 2), -1e6, requires_grad=False, dtype=dtype, device=device)\n",
    "\n",
    "    output = gaussian_weight.apply(gaussian_mean, inverted_covariance, pixel)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    assert torch.isfinite(gaussian_mean.grad).all(), \"Gradients for gaussian_mean are not finite with extreme values!\"\n",
    "    assert torch.isfinite(inverted_covariance.grad).all(), \"Gradients for inverted_covariance are not finite with extreme values!\"\n",
    "    print(\"Extreme value test passed.\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_output torch.Size([4, 3])\n",
      "custom mean_3d grad tensor([[0.7980, 1.8702, 1.5987],\n",
      "        [0.7980, 1.8702, 1.5987],\n",
      "        [0.7980, 1.8702, 1.5987],\n",
      "        [0.7980, 1.8702, 1.5987]], device='cuda:0')\n",
      "autograd mean_3d grad tensor([[0.7980, 1.8702, 1.5987],\n",
      "        [0.7980, 1.8702, 1.5987],\n",
      "        [0.7980, 1.8702, 1.5987],\n",
      "        [0.7980, 1.8702, 1.5987]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "from splat.custom_backwards_implementation.gaussian_weight_derivatives import mean_3d_to_camera_space_backward_launcher\n",
    "\n",
    "\n",
    "class mean_3d_to_camera_space(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, mean_3d: torch.Tensor, extrinsic_matrix: torch.Tensor):\n",
    "        ctx.save_for_backward(mean_3d, extrinsic_matrix)\n",
    "        return torch.einsum(\"nk, kh->nh\", mean_3d, extrinsic_matrix)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        print(\"grad_output\", grad_output.shape)\n",
    "        mean_3d, extrinsic_matrix = ctx.saved_tensors\n",
    "        grad_mean_3d = torch.zeros_like(mean_3d)\n",
    "        mean_3d_to_camera_space_backward_launcher(\n",
    "            grad_output.contiguous(), \n",
    "            extrinsic_matrix.contiguous(), \n",
    "            grad_mean_3d.contiguous()\n",
    "        )\n",
    "        return grad_mean_3d, None\n",
    "\n",
    "# Helper function to compare gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# Define a test function using PyTorch's autograd\n",
    "def test_mean_3d_to_camera_space(mean_3d, extrinsic_matrix):\n",
    "    return torch.einsum(\"nk, kh->nh\", mean_3d, extrinsic_matrix)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32\n",
    "\n",
    "mean_3d = torch.randn((4, 3), requires_grad=True, dtype=dtype, device=device)\n",
    "extrinsic_matrix = torch.rand(3, 3, requires_grad=False, dtype=dtype, device=device)\n",
    "\n",
    "output = mean_3d_to_camera_space.apply(mean_3d, extrinsic_matrix)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(\"custom mean_3d grad\", mean_3d.grad)\n",
    "\n",
    "mean_3d.grad.zero_()\n",
    "output = test_mean_3d_to_camera_space(mean_3d, extrinsic_matrix)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(\"autograd mean_3d grad\", mean_3d.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed for mean_3d_to_camera_space.\n",
      "Gradients match for autograd.\n",
      "Edge case test passed for zero tensors.\n",
      "Gradients match for large batch size.\n",
      "Extreme value test passed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "from splat.custom_backwards_implementation.gaussian_weight_derivatives import mean_3d_to_camera_space_backward_launcher\n",
    "\n",
    "\n",
    "class mean_3d_to_camera_space(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, mean_3d: torch.Tensor, extrinsic_matrix: torch.Tensor):\n",
    "        ctx.save_for_backward(mean_3d, extrinsic_matrix)\n",
    "        return torch.einsum(\"nk, kh->nh\", mean_3d, extrinsic_matrix)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        mean_3d, extrinsic_matrix = ctx.saved_tensors\n",
    "        grad_mean_3d = torch.zeros_like(mean_3d)\n",
    "        mean_3d_to_camera_space_backward_launcher(\n",
    "            grad_output.contiguous(), \n",
    "            extrinsic_matrix.contiguous(), \n",
    "            grad_mean_3d.contiguous()\n",
    "        )\n",
    "        return grad_mean_3d, None\n",
    "\n",
    "# Helper function to compare gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# Define a test function using PyTorch's autograd\n",
    "def test_mean_3d_to_camera_space(mean_3d, extrinsic_matrix):\n",
    "    return torch.einsum(\"nk, kh->nh\", mean_3d, extrinsic_matrix)\n",
    "\n",
    "def run_tests():\n",
    "    tolerance = 1e-4\n",
    "    eps = 1e-3\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dtype = torch.float32\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    mean_3d = torch.randn((5, 3), requires_grad=True, dtype=torch.float32, device=device)\n",
    "    extrinsic_matrix = torch.eye(3, requires_grad=False, dtype=torch.float32, device=device)\n",
    "\n",
    "    assert gradcheck(mean_3d_to_camera_space.apply, (mean_3d, extrinsic_matrix), eps=eps), \"Gradcheck failed for mean_3d_to_camera_space!\"\n",
    "    print(\"Gradcheck passed for mean_3d_to_camera_space.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd\n",
    "    mean_3d = torch.randn((5, 3), requires_grad=True, dtype=dtype, device=device)\n",
    "    extrinsic_matrix = torch.eye(3, requires_grad=False, dtype=dtype, device=device)\n",
    "\n",
    "    # My grads\n",
    "    output = mean_3d_to_camera_space.apply(mean_3d, extrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_mean_3d = mean_3d.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    mean_3d.grad.zero_()\n",
    "    output = test_mean_3d_to_camera_space(mean_3d, extrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_mean_3d = mean_3d.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_mean_3d, autograd_grads_mean_3d, tolerance), \"Mismatch in gradients for mean_3d!\"\n",
    "    print(\"Gradients match for autograd.\")\n",
    "\n",
    "    # 3. Test edge cases: zero tensors\n",
    "    mean_3d = torch.zeros((5, 3), requires_grad=True, dtype=dtype, device=device)\n",
    "    extrinsic_matrix = torch.eye(3, requires_grad=False, dtype=dtype, device=device)\n",
    "\n",
    "    output = mean_3d_to_camera_space.apply(mean_3d, extrinsic_matrix)\n",
    "    assert torch.all(output == 0), \"Output is not zero for zero input tensors.\"\n",
    "    print(\"Edge case test passed for zero tensors.\")\n",
    "\n",
    "    # 4. Test with large batch sizes\n",
    "    batch_size = 100\n",
    "    mean_3d = torch.randn((batch_size, 3), requires_grad=True, dtype=dtype, device=device)\n",
    "    extrinsic_matrix = torch.eye(3, requires_grad=False, dtype=dtype, device=device)\n",
    "\n",
    "    # My grads\n",
    "    output = mean_3d_to_camera_space.apply(mean_3d, extrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_mean_3d = mean_3d.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    mean_3d.grad.zero_()\n",
    "    output = test_mean_3d_to_camera_space(mean_3d, extrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_mean_3d = mean_3d.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_mean_3d, autograd_grads_mean_3d, tolerance), \"Mismatch in gradients for mean_3d (large batch size)!\"\n",
    "    print(\"Gradients match for large batch size.\")\n",
    "\n",
    "    # 5. Test with extreme values\n",
    "    mean_3d = torch.full((5, 3), 1e6, requires_grad=True, dtype=dtype, device=device)\n",
    "    extrinsic_matrix = torch.eye(3, requires_grad=False, dtype=dtype, device=device)\n",
    "\n",
    "    output = mean_3d_to_camera_space.apply(mean_3d, extrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    assert torch.isfinite(mean_3d.grad).all(), \"Gradients for mean_3d are not finite with extreme values!\"\n",
    "    print(\"Extreme value test passed.\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed for camera_space_to_pixel_space.\n",
      "Gradients match for autograd.\n",
      "Edge case test passed for zero tensors.\n",
      "Gradients match for large batch size.\n",
      "Extreme value test passed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "from splat.custom_backwards_implementation.gaussian_weight_derivatives import camera_space_to_pixel_space_backward_launcher\n",
    "\n",
    "\n",
    "class camera_space_to_pixel_space(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, mean_3d: torch.Tensor, intrinsic_matrix: torch.Tensor):\n",
    "        ctx.save_for_backward(intrinsic_matrix, mean_3d)\n",
    "        return torch.einsum(\"nk, kh->nh\", mean_3d, intrinsic_matrix)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):   \n",
    "        intrinsic_matrix, mean_3d = ctx.saved_tensors\n",
    "        mean_3d_grad = torch.zeros_like(mean_3d)\n",
    "        camera_space_to_pixel_space_backward_launcher(\n",
    "            grad_output.contiguous(), \n",
    "            intrinsic_matrix.contiguous(), \n",
    "            mean_3d_grad.contiguous()\n",
    "        )\n",
    "        return mean_3d_grad, None\n",
    "\n",
    "# Helper function to compare gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# Define a test function using PyTorch's autograd\n",
    "def test_camera_space_to_pixel_space(mean_3d, intrinsic_matrix):\n",
    "    return torch.einsum(\"nk, kh->nh\", mean_3d, intrinsic_matrix)\n",
    "\n",
    "\n",
    "def run_tests():\n",
    "    tolerance = 1e-4\n",
    "    eps = 1e-3\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dtype = torch.float32\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    mean_3d = torch.randn((5, 3), requires_grad=True, dtype=torch.float32, device=device)\n",
    "    intrinsic_matrix = torch.eye(3, requires_grad=False, dtype=torch.float32, device=device)\n",
    "\n",
    "    assert gradcheck(camera_space_to_pixel_space.apply, (mean_3d, intrinsic_matrix), eps=eps), \"Gradcheck failed for camera_space_to_pixel_space!\"\n",
    "    print(\"Gradcheck passed for camera_space_to_pixel_space.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd\n",
    "    mean_3d = torch.randn((5, 3), requires_grad=True, dtype=dtype, device=device)\n",
    "    intrinsic_matrix = torch.eye(3, requires_grad=False, dtype=dtype, device=device)\n",
    "\n",
    "    # My grads\n",
    "    output = camera_space_to_pixel_space.apply(mean_3d, intrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_mean_3d = mean_3d.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    mean_3d.grad.zero_()\n",
    "    output = test_camera_space_to_pixel_space(mean_3d, intrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_mean_3d = mean_3d.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_mean_3d, autograd_grads_mean_3d, tolerance), \"Mismatch in gradients for mean_3d!\"\n",
    "    print(\"Gradients match for autograd.\")\n",
    "\n",
    "    # 3. Test edge cases: zero tensors\n",
    "    mean_3d = torch.zeros((5, 3), requires_grad=True, dtype=dtype, device=device)\n",
    "    intrinsic_matrix = torch.eye(3, requires_grad=False, dtype=dtype, device=device)\n",
    "\n",
    "    output = camera_space_to_pixel_space.apply(mean_3d, intrinsic_matrix)\n",
    "    assert torch.all(output == 0), \"Output is not zero for zero input tensors.\"\n",
    "    print(\"Edge case test passed for zero tensors.\")\n",
    "\n",
    "    # 4. Test with large batch sizes\n",
    "    batch_size = 100\n",
    "    mean_3d = torch.randn((batch_size, 3), requires_grad=True, dtype=dtype, device=device)\n",
    "    intrinsic_matrix = torch.eye(3, requires_grad=False, dtype=dtype, device=device)\n",
    "\n",
    "    # My grads\n",
    "    output = camera_space_to_pixel_space.apply(mean_3d, intrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_mean_3d = mean_3d.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    mean_3d.grad.zero_()\n",
    "    output = test_camera_space_to_pixel_space(mean_3d, intrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_mean_3d = mean_3d.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_mean_3d, autograd_grads_mean_3d, tolerance), \"Mismatch in gradients for mean_3d (large batch size)!\"\n",
    "    print(\"Gradients match for large batch size.\")\n",
    "\n",
    "    # 5. Test with extreme values\n",
    "    mean_3d = torch.full((5, 3), 1e6, requires_grad=True, dtype=dtype, device=device)\n",
    "    intrinsic_matrix = torch.eye(3, requires_grad=False, dtype=dtype, device=device)\n",
    "\n",
    "    output = camera_space_to_pixel_space.apply(mean_3d, intrinsic_matrix)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    assert torch.isfinite(mean_3d.grad).all(), \"Gradients for mean_3d are not finite with extreme values!\"\n",
    "    print(\"Extreme value test passed.\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da2986/intro_to_gaussian_splatting/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passed for ndc_to_pixels.\n",
      "Gradients match for autograd.\n",
      "Edge case test passed for zero tensors.\n",
      "Gradients match for large batch size.\n",
      "Extreme value test passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da2986/intro_to_gaussian_splatting/.venv/lib/python3.10/site-packages/torch/autograd/gradcheck.py:652: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. \n",
      "  warnings.warn(\n",
      "/var/tmp/ipykernel_21651/1931381568.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ctx.save_for_backward(torch.tensor(dimension, dtype=torch.float32), ndc)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "from splat.custom_backwards_implementation.gaussian_weight_derivatives import ndc_to_pixels_backward_launcher\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "class ndc_to_pixels(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, ndc: torch.Tensor, dimension: list):\n",
    "        \"\"\"ndc is a nx3 tensor where the last dimension is the z component\n",
    "        \n",
    "        dimension are the height and width of the image\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(torch.tensor(dimension, dtype=torch.float32), ndc)\n",
    "        ndc = ndc.clone()  # To avoid modifying input in-place\n",
    "        ndc[:, 0] = (ndc[:, 0] + 1) * (dimension[1] - 1) * 0.5\n",
    "        ndc[:, 1] = (ndc[:, 1] + 1) * (dimension[0] - 1) * 0.5\n",
    "        return ndc\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        dimension, ndc = ctx.saved_tensors\n",
    "        grad_ndc = torch.zeros_like(ndc)\n",
    "        ndc_to_pixels_backward_launcher(\n",
    "            grad_output.contiguous(), \n",
    "            dimension.contiguous(), \n",
    "            grad_ndc.contiguous()\n",
    "        )\n",
    "        return grad_ndc, None\n",
    "\n",
    "# Helper function to compare gradients\n",
    "def compare_grads(my_grads, autograd_grads, tolerance=1e-6):\n",
    "    return torch.allclose(my_grads, autograd_grads, atol=tolerance, rtol=tolerance)\n",
    "\n",
    "# Define a test function using PyTorch's autograd\n",
    "def test_ndc_to_pixels(ndc: torch.Tensor, dimension: list):\n",
    "    ndc = ndc.clone()\n",
    "    ndc[:, 0] = (ndc[:, 0] + 1) * (dimension[1] - 1) * 0.5\n",
    "    ndc[:, 1] = (ndc[:, 1] + 1) * (dimension[0] - 1) * 0.5\n",
    "    return ndc\n",
    "\n",
    "def run_tests():\n",
    "    tolerance = 1e-2\n",
    "    eps = 1e-3\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dtype = torch.float32\n",
    "\n",
    "    # 1. Gradcheck for numerical gradient correctness\n",
    "    ndc = torch.randn((5, 3), requires_grad=True, dtype=torch.float32, device=device)\n",
    "    dimension = torch.tensor([480, 640], dtype=torch.float32, device=device)\n",
    "\n",
    "    assert gradcheck(ndc_to_pixels.apply, (ndc, dimension), eps=eps), \"Gradcheck failed for ndc_to_pixels!\"\n",
    "    print(\"Gradcheck passed for ndc_to_pixels.\")\n",
    "\n",
    "    # 2. Verify backward computation with autograd\n",
    "    ndc = torch.randn((5, 3), requires_grad=True, dtype=dtype, device=device)\n",
    "\n",
    "    # My grads\n",
    "    output = ndc_to_pixels.apply(ndc, dimension)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_ndc = ndc.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    ndc.grad.zero_()\n",
    "    output = test_ndc_to_pixels(ndc, dimension)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_ndc = ndc.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_ndc, autograd_grads_ndc, tolerance), \"Mismatch in gradients for ndc!\"\n",
    "    print(\"Gradients match for autograd.\")\n",
    "\n",
    "    # 3. Test edge cases: zero tensors\n",
    "    ndc = torch.zeros((5, 3), requires_grad=True, dtype=dtype, device=device)\n",
    "\n",
    "    output = ndc_to_pixels.apply(ndc, dimension)\n",
    "    expected_output = torch.zeros_like(ndc)\n",
    "    expected_output[:, 0] = 0.5 * (dimension[1] - 1)\n",
    "    expected_output[:, 1] = 0.5 * (dimension[0] - 1)\n",
    "    assert torch.allclose(output, expected_output, atol=tolerance, rtol=tolerance), \"Output mismatch for zero input tensors.\"\n",
    "    print(\"Edge case test passed for zero tensors.\")\n",
    "\n",
    "    # 4. Test with large batch sizes\n",
    "    batch_size = 100\n",
    "    ndc = torch.randn((batch_size, 3), requires_grad=True, dtype=dtype, device=device)\n",
    "\n",
    "    # My grads\n",
    "    output = ndc_to_pixels.apply(ndc, dimension)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    my_grads_ndc = ndc.grad.clone()\n",
    "\n",
    "    # Autograd grads\n",
    "    ndc.grad.zero_()\n",
    "    output = test_ndc_to_pixels(ndc, dimension)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    autograd_grads_ndc = ndc.grad.clone()\n",
    "\n",
    "    assert compare_grads(my_grads_ndc, autograd_grads_ndc, tolerance), \"Mismatch in gradients for ndc (large batch size)!\"\n",
    "    print(\"Gradients match for large batch size.\")\n",
    "\n",
    "    # 5. Test with extreme values\n",
    "    ndc = torch.full((10, 3), 1e6, requires_grad=True, dtype=dtype, device=device)\n",
    "\n",
    "    output = ndc_to_pixels.apply(ndc, dimension)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    assert torch.isfinite(ndc.grad).all(), \"Gradients for ndc are not finite with extreme values!\"\n",
    "    print(\"Extreme value test passed.\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
