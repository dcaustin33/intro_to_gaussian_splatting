{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "from splat.utils import build_rotation\n",
    "from splat.custom_backwards_implementation.gaussian_weight_derivatives import *\n",
    "\n",
    "\n",
    "class final_color(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, color: torch.Tensor, current_T: torch.Tensor, alpha: torch.Tensor):\n",
    "        \"\"\"Color is a nx3 tensor, weight is a nx1 tensor, alpha is a nx1 tensor\"\"\"\n",
    "        ctx.save_for_backward(color, current_T, alpha)\n",
    "        return color * current_T * alpha\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        \"\"\"Output of forward is a nx3 tensor so the grad_output is a nx3 tensor\"\"\"\n",
    "        color, current_T, alpha = ctx.saved_tensors\n",
    "        grad_color = grad_output * current_T * alpha\n",
    "        grad_alpha = (grad_output * color * current_T).sum(dim=1, keepdim=True)\n",
    "        return grad_color, None, grad_alpha\n",
    "    \n",
    "def autograd_test(color: torch.Tensor, current_T: torch.Tensor, alpha: torch.Tensor):\n",
    "    return color * current_T * alpha\n",
    "\n",
    "# first we check with gradcheck\n",
    "color = torch.tensor([[1.0, 2.0, 3.0]], requires_grad=True, dtype=torch.float64)\n",
    "current_T = torch.tensor([[4.0]], dtype=torch.float64)\n",
    "alpha = torch.tensor([[5.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "print(\"gradcheck passing: \", gradcheck(final_color.apply, (color, current_T, alpha)))\n",
    "\n",
    "# then we check with autograd\n",
    "color = torch.tensor([[1.0, 2.0, 3.0]], requires_grad=True, dtype=torch.float64)\n",
    "current_T = torch.tensor([[4.0]], dtype=torch.float64)\n",
    "alpha = torch.tensor([[5.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "output = final_color.apply(color, current_T, alpha)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(\"My grads: \", color.grad, current_T.grad, alpha.grad)\n",
    "\n",
    "# then we check with autograd\n",
    "color = torch.tensor([[1.0, 2.0, 3.0]], requires_grad=True, dtype=torch.float64)\n",
    "current_T = torch.tensor([[4.0]], dtype=torch.float64)\n",
    "alpha = torch.tensor([[5.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "output = autograd_test(color, current_T, alpha)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(\"Autograd grads: \", color.grad, current_T.grad, alpha.grad)\n",
    "\n",
    "\n",
    "# finally we test with multiple dimensions\n",
    "color = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], requires_grad=True, dtype=torch.float64)\n",
    "current_T = torch.tensor([[4.0], [5.0]], dtype=torch.float64)\n",
    "alpha = torch.tensor([[5.0], [6.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "output = final_color.apply(color, current_T, alpha)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(\"My grads: \", color.grad, current_T.grad, alpha.grad)\n",
    "\n",
    "color = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], requires_grad=True, dtype=torch.float64)\n",
    "current_T = torch.tensor([[4.0], [5.0]], dtype=torch.float64)\n",
    "alpha = torch.tensor([[5.0], [6.0]], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "output = autograd_test(color, current_T, alpha)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(\"Autograd grads: \", color.grad, current_T.grad, alpha.grad)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
